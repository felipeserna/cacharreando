{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8JBflzmhaKGz84sbsLPrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipeserna/cacharreando/blob/master/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12HP8K2VJBMF"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ua8igzLJKNc"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sRiDnEJRQO"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGj_H0SJWwu",
        "outputId": "3be69659-a968-4ac0-f6b3-9d97a38eb23d"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fb63f098210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85nZ8_g2J7Zd"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars\n",
        "!pip install keras-rl2\n",
        "!pip install gym\n",
        "!pip install gym[atari]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCLPqmxSKnW4",
        "outputId": "c5c93baa-b8cf-4650-f518-545b9e8faef6"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train.py\n",
        "Script that utilizes keras, keras-rl, and gym\n",
        "to train an agent that can play Atari’s Breakout:\n",
        "* Use keras-rl‘s DQNAgent, SequentialMemory, and EpsGreedyQPolicy\n",
        "* Save the final policy network as policy.h5\n",
        "\"\"\"\n",
        "import gym\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.processors import Processor\n",
        "import tensorflow.keras as K\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    \"\"\"\n",
        "    The environment in which the game will be played.\n",
        "    Processor for Atari.\n",
        "    Prepocesses data based on Deep Learning\n",
        "    Quick Reference by Mike Bernico.\n",
        "    \"\"\"\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\"\n",
        "        Resizing and grayscale\n",
        "        \"\"\"\n",
        "        # (height, width, channel)\n",
        "        assert observation.ndim == 3\n",
        "        # Retrieve image from array\n",
        "        img = Image.fromarray(observation)\n",
        "        # Resize image and convert to grayscale\n",
        "        img = img.resize((84, 84), Image.ANTIALIAS).convert('L')\n",
        "        # Convert back to array\n",
        "        processed_observation = np.array(img)\n",
        "        # Assert input shape\n",
        "        assert processed_observation.shape == (84, 84)\n",
        "        \n",
        "        # Save processed observation in experience memory\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Convert the batch of images to float32\n",
        "        \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\"\n",
        "        Rewards between -1 and 1\n",
        "        \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "\n",
        "def create_q_model(num_actions, window):\n",
        "    \"\"\"\n",
        "    CNN with Keras defined by the Deepmind paper\n",
        "    \"\"\"\n",
        "    # Each RL state is composed of 4 windows\n",
        "    inputs = layers.Input(shape=(window, 84, 84))\n",
        "    # Permute is used to change the dimensions of the input\n",
        "    # according to a given pattern\n",
        "    layer0 = layers.Permute((2, 3, 1))(inputs)\n",
        "\n",
        "    layer1 = layers.Conv2D(filters=32, kernel_size=8, strides=(4, 4),\n",
        "                           activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer0)\n",
        "    layer2 = layers.Conv2D(filters=64, kernel_size=4, strides=(2, 2),\n",
        "                           activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer1)\n",
        "    layer3 = layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1),\n",
        "                           activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return K.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    window = 4\n",
        "    model = create_q_model(num_actions, window)\n",
        "    model.summary()\n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                  value_max=1., value_min=.1, value_test=.05,\n",
        "                                  nb_steps=1000000)\n",
        "\n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions, policy=policy,\n",
        "                   memory=memory, processor=processor,\n",
        "                   nb_steps_warmup=50000, gamma=.99,\n",
        "                   target_model_update=10000,\n",
        "                   train_interval=4,\n",
        "                   delta_clip=1.)\n",
        "\n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "    \n",
        "    # Train the model\n",
        "    dqn.fit(env,\n",
        "            nb_steps=100000,\n",
        "            log_interval=10000,\n",
        "            visualize=False,\n",
        "            verbose=2)\n",
        "\n",
        "    # Save the final policy network\n",
        "    dqn.save_weights('policy.h5', overwrite=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4, 84, 84)]       0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,686,180\n",
            "Trainable params: 1,686,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 100000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   175/100000: episode: 1, duration: 29.706s, episode steps: 175, steps per second:   6, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   353/100000: episode: 2, duration: 0.624s, episode steps: 178, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   633/100000: episode: 3, duration: 0.946s, episode steps: 280, steps per second: 296, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   967/100000: episode: 4, duration: 1.123s, episode steps: 334, steps per second: 297, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1133/100000: episode: 5, duration: 0.566s, episode steps: 166, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1317/100000: episode: 6, duration: 0.644s, episode steps: 184, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1502/100000: episode: 7, duration: 0.628s, episode steps: 185, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1741/100000: episode: 8, duration: 0.806s, episode steps: 239, steps per second: 297, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1957/100000: episode: 9, duration: 0.734s, episode steps: 216, steps per second: 294, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2131/100000: episode: 10, duration: 0.594s, episode steps: 174, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2295/100000: episode: 11, duration: 0.560s, episode steps: 164, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2531/100000: episode: 12, duration: 0.806s, episode steps: 236, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2716/100000: episode: 13, duration: 0.627s, episode steps: 185, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2887/100000: episode: 14, duration: 0.582s, episode steps: 171, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3059/100000: episode: 15, duration: 0.584s, episode steps: 172, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3423/100000: episode: 16, duration: 1.230s, episode steps: 364, steps per second: 296, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3862/100000: episode: 17, duration: 1.476s, episode steps: 439, steps per second: 297, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4039/100000: episode: 18, duration: 0.611s, episode steps: 177, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4285/100000: episode: 19, duration: 0.861s, episode steps: 246, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4519/100000: episode: 20, duration: 0.845s, episode steps: 234, steps per second: 277, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4767/100000: episode: 21, duration: 0.853s, episode steps: 248, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5051/100000: episode: 22, duration: 0.969s, episode steps: 284, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5233/100000: episode: 23, duration: 0.617s, episode steps: 182, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5579/100000: episode: 24, duration: 1.166s, episode steps: 346, steps per second: 297, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5762/100000: episode: 25, duration: 0.627s, episode steps: 183, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5997/100000: episode: 26, duration: 0.808s, episode steps: 235, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6183/100000: episode: 27, duration: 0.645s, episode steps: 186, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6453/100000: episode: 28, duration: 0.922s, episode steps: 270, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6714/100000: episode: 29, duration: 0.891s, episode steps: 261, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6892/100000: episode: 30, duration: 0.620s, episode steps: 178, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7162/100000: episode: 31, duration: 0.925s, episode steps: 270, steps per second: 292, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7331/100000: episode: 32, duration: 0.595s, episode steps: 169, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7520/100000: episode: 33, duration: 0.660s, episode steps: 189, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7702/100000: episode: 34, duration: 0.623s, episode steps: 182, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7951/100000: episode: 35, duration: 0.843s, episode steps: 249, steps per second: 295, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8133/100000: episode: 36, duration: 0.621s, episode steps: 182, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8309/100000: episode: 37, duration: 0.601s, episode steps: 176, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8495/100000: episode: 38, duration: 0.638s, episode steps: 186, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8777/100000: episode: 39, duration: 0.959s, episode steps: 282, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8944/100000: episode: 40, duration: 0.586s, episode steps: 167, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9187/100000: episode: 41, duration: 0.851s, episode steps: 243, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9345/100000: episode: 42, duration: 0.548s, episode steps: 158, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9552/100000: episode: 43, duration: 0.708s, episode steps: 207, steps per second: 292, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9848/100000: episode: 44, duration: 1.014s, episode steps: 296, steps per second: 292, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10167/100000: episode: 45, duration: 1.105s, episode steps: 319, steps per second: 289, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10537/100000: episode: 46, duration: 1.294s, episode steps: 370, steps per second: 286, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10780/100000: episode: 47, duration: 0.822s, episode steps: 243, steps per second: 295, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11007/100000: episode: 48, duration: 0.782s, episode steps: 227, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11180/100000: episode: 49, duration: 0.592s, episode steps: 173, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11477/100000: episode: 50, duration: 1.010s, episode steps: 297, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11655/100000: episode: 51, duration: 0.615s, episode steps: 178, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11832/100000: episode: 52, duration: 0.613s, episode steps: 177, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12110/100000: episode: 53, duration: 0.944s, episode steps: 278, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12307/100000: episode: 54, duration: 0.669s, episode steps: 197, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12592/100000: episode: 55, duration: 0.967s, episode steps: 285, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12866/100000: episode: 56, duration: 0.931s, episode steps: 274, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13044/100000: episode: 57, duration: 0.612s, episode steps: 178, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13316/100000: episode: 58, duration: 0.937s, episode steps: 272, steps per second: 290, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13488/100000: episode: 59, duration: 0.591s, episode steps: 172, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13736/100000: episode: 60, duration: 0.851s, episode steps: 248, steps per second: 292, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13967/100000: episode: 61, duration: 0.806s, episode steps: 231, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14201/100000: episode: 62, duration: 0.801s, episode steps: 234, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14420/100000: episode: 63, duration: 0.744s, episode steps: 219, steps per second: 294, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14589/100000: episode: 64, duration: 0.585s, episode steps: 169, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14774/100000: episode: 65, duration: 0.632s, episode steps: 185, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14992/100000: episode: 66, duration: 0.740s, episode steps: 218, steps per second: 295, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15340/100000: episode: 67, duration: 1.194s, episode steps: 348, steps per second: 292, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15622/100000: episode: 68, duration: 0.955s, episode steps: 282, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15832/100000: episode: 69, duration: 0.729s, episode steps: 210, steps per second: 288, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16001/100000: episode: 70, duration: 0.584s, episode steps: 169, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16186/100000: episode: 71, duration: 0.644s, episode steps: 185, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.622 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16409/100000: episode: 72, duration: 0.772s, episode steps: 223, steps per second: 289, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16636/100000: episode: 73, duration: 0.774s, episode steps: 227, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16802/100000: episode: 74, duration: 0.562s, episode steps: 166, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17012/100000: episode: 75, duration: 0.760s, episode steps: 210, steps per second: 276, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17212/100000: episode: 76, duration: 0.719s, episode steps: 200, steps per second: 278, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17399/100000: episode: 77, duration: 0.654s, episode steps: 187, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17601/100000: episode: 78, duration: 0.681s, episode steps: 202, steps per second: 297, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17820/100000: episode: 79, duration: 0.753s, episode steps: 219, steps per second: 291, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18147/100000: episode: 80, duration: 1.115s, episode steps: 327, steps per second: 293, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18397/100000: episode: 81, duration: 0.850s, episode steps: 250, steps per second: 294, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18708/100000: episode: 82, duration: 1.053s, episode steps: 311, steps per second: 295, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18886/100000: episode: 83, duration: 0.625s, episode steps: 178, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19067/100000: episode: 84, duration: 0.634s, episode steps: 181, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19284/100000: episode: 85, duration: 0.732s, episode steps: 217, steps per second: 297, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19462/100000: episode: 86, duration: 0.610s, episode steps: 178, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19817/100000: episode: 87, duration: 1.196s, episode steps: 355, steps per second: 297, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20249/100000: episode: 88, duration: 1.457s, episode steps: 432, steps per second: 296, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20564/100000: episode: 89, duration: 1.063s, episode steps: 315, steps per second: 296, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20731/100000: episode: 90, duration: 0.591s, episode steps: 167, steps per second: 283, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21080/100000: episode: 91, duration: 1.177s, episode steps: 349, steps per second: 296, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21367/100000: episode: 92, duration: 0.973s, episode steps: 287, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21646/100000: episode: 93, duration: 0.945s, episode steps: 279, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21923/100000: episode: 94, duration: 0.957s, episode steps: 277, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22083/100000: episode: 95, duration: 0.576s, episode steps: 160, steps per second: 278, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22560/100000: episode: 96, duration: 1.602s, episode steps: 477, steps per second: 298, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22793/100000: episode: 97, duration: 0.795s, episode steps: 233, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23101/100000: episode: 98, duration: 1.042s, episode steps: 308, steps per second: 296, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23280/100000: episode: 99, duration: 0.608s, episode steps: 179, steps per second: 295, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23467/100000: episode: 100, duration: 0.636s, episode steps: 187, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.668 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23768/100000: episode: 101, duration: 1.028s, episode steps: 301, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23978/100000: episode: 102, duration: 0.714s, episode steps: 210, steps per second: 294, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24399/100000: episode: 103, duration: 1.407s, episode steps: 421, steps per second: 299, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24577/100000: episode: 104, duration: 0.615s, episode steps: 178, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24745/100000: episode: 105, duration: 0.575s, episode steps: 168, steps per second: 292, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25005/100000: episode: 106, duration: 0.883s, episode steps: 260, steps per second: 295, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25194/100000: episode: 107, duration: 0.666s, episode steps: 189, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25541/100000: episode: 108, duration: 1.167s, episode steps: 347, steps per second: 297, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25700/100000: episode: 109, duration: 0.552s, episode steps: 159, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25971/100000: episode: 110, duration: 0.928s, episode steps: 271, steps per second: 292, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26255/100000: episode: 111, duration: 0.963s, episode steps: 284, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.320 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26537/100000: episode: 112, duration: 0.951s, episode steps: 282, steps per second: 297, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26702/100000: episode: 113, duration: 0.568s, episode steps: 165, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27014/100000: episode: 114, duration: 1.070s, episode steps: 312, steps per second: 291, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27180/100000: episode: 115, duration: 0.574s, episode steps: 166, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27412/100000: episode: 116, duration: 0.799s, episode steps: 232, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27690/100000: episode: 117, duration: 0.947s, episode steps: 278, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27915/100000: episode: 118, duration: 0.770s, episode steps: 225, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28131/100000: episode: 119, duration: 0.742s, episode steps: 216, steps per second: 291, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28357/100000: episode: 120, duration: 0.791s, episode steps: 226, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28593/100000: episode: 121, duration: 0.811s, episode steps: 236, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28776/100000: episode: 122, duration: 0.626s, episode steps: 183, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29042/100000: episode: 123, duration: 0.903s, episode steps: 266, steps per second: 295, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29217/100000: episode: 124, duration: 0.606s, episode steps: 175, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29529/100000: episode: 125, duration: 1.062s, episode steps: 312, steps per second: 294, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29721/100000: episode: 126, duration: 0.659s, episode steps: 192, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29987/100000: episode: 127, duration: 0.904s, episode steps: 266, steps per second: 294, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30287/100000: episode: 128, duration: 1.020s, episode steps: 300, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30628/100000: episode: 129, duration: 1.158s, episode steps: 341, steps per second: 295, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30929/100000: episode: 130, duration: 1.012s, episode steps: 301, steps per second: 297, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31183/100000: episode: 131, duration: 0.858s, episode steps: 254, steps per second: 296, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31372/100000: episode: 132, duration: 0.642s, episode steps: 189, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.487 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31605/100000: episode: 133, duration: 0.796s, episode steps: 233, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31883/100000: episode: 134, duration: 0.956s, episode steps: 278, steps per second: 291, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32049/100000: episode: 135, duration: 0.575s, episode steps: 166, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32274/100000: episode: 136, duration: 0.779s, episode steps: 225, steps per second: 289, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32477/100000: episode: 137, duration: 0.717s, episode steps: 203, steps per second: 283, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32709/100000: episode: 138, duration: 0.802s, episode steps: 232, steps per second: 289, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33213/100000: episode: 139, duration: 1.720s, episode steps: 504, steps per second: 293, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33532/100000: episode: 140, duration: 1.062s, episode steps: 319, steps per second: 300, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33793/100000: episode: 141, duration: 0.900s, episode steps: 261, steps per second: 290, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33963/100000: episode: 142, duration: 0.596s, episode steps: 170, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34153/100000: episode: 143, duration: 0.665s, episode steps: 190, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34365/100000: episode: 144, duration: 0.722s, episode steps: 212, steps per second: 293, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34580/100000: episode: 145, duration: 0.735s, episode steps: 215, steps per second: 293, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34766/100000: episode: 146, duration: 0.628s, episode steps: 186, steps per second: 296, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35017/100000: episode: 147, duration: 0.868s, episode steps: 251, steps per second: 289, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35186/100000: episode: 148, duration: 0.575s, episode steps: 169, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35401/100000: episode: 149, duration: 0.722s, episode steps: 215, steps per second: 298, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35771/100000: episode: 150, duration: 1.253s, episode steps: 370, steps per second: 295, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36105/100000: episode: 151, duration: 1.119s, episode steps: 334, steps per second: 298, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36442/100000: episode: 152, duration: 1.200s, episode steps: 337, steps per second: 281, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36784/100000: episode: 153, duration: 1.168s, episode steps: 342, steps per second: 293, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36970/100000: episode: 154, duration: 0.642s, episode steps: 186, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37241/100000: episode: 155, duration: 0.945s, episode steps: 271, steps per second: 287, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37563/100000: episode: 156, duration: 1.091s, episode steps: 322, steps per second: 295, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37746/100000: episode: 157, duration: 0.625s, episode steps: 183, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38057/100000: episode: 158, duration: 1.070s, episode steps: 311, steps per second: 291, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38434/100000: episode: 159, duration: 1.280s, episode steps: 377, steps per second: 295, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38673/100000: episode: 160, duration: 0.823s, episode steps: 239, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38898/100000: episode: 161, duration: 0.771s, episode steps: 225, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39076/100000: episode: 162, duration: 0.614s, episode steps: 178, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39430/100000: episode: 163, duration: 1.210s, episode steps: 354, steps per second: 293, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39677/100000: episode: 164, duration: 0.850s, episode steps: 247, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39953/100000: episode: 165, duration: 0.962s, episode steps: 276, steps per second: 287, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40405/100000: episode: 166, duration: 1.520s, episode steps: 452, steps per second: 297, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40640/100000: episode: 167, duration: 0.807s, episode steps: 235, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40808/100000: episode: 168, duration: 0.582s, episode steps: 168, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41086/100000: episode: 169, duration: 0.954s, episode steps: 278, steps per second: 292, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41345/100000: episode: 170, duration: 0.894s, episode steps: 259, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41652/100000: episode: 171, duration: 1.027s, episode steps: 307, steps per second: 299, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41972/100000: episode: 172, duration: 1.070s, episode steps: 320, steps per second: 299, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42264/100000: episode: 173, duration: 0.990s, episode steps: 292, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42449/100000: episode: 174, duration: 0.629s, episode steps: 185, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42626/100000: episode: 175, duration: 0.608s, episode steps: 177, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42937/100000: episode: 176, duration: 1.043s, episode steps: 311, steps per second: 298, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43242/100000: episode: 177, duration: 1.041s, episode steps: 305, steps per second: 293, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43417/100000: episode: 178, duration: 0.597s, episode steps: 175, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43598/100000: episode: 179, duration: 0.617s, episode steps: 181, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43769/100000: episode: 180, duration: 0.603s, episode steps: 171, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44015/100000: episode: 181, duration: 0.850s, episode steps: 246, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44193/100000: episode: 182, duration: 0.608s, episode steps: 178, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44362/100000: episode: 183, duration: 0.589s, episode steps: 169, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44557/100000: episode: 184, duration: 0.666s, episode steps: 195, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44730/100000: episode: 185, duration: 0.594s, episode steps: 173, steps per second: 291, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44971/100000: episode: 186, duration: 0.830s, episode steps: 241, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45282/100000: episode: 187, duration: 1.057s, episode steps: 311, steps per second: 294, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45530/100000: episode: 188, duration: 0.848s, episode steps: 248, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45816/100000: episode: 189, duration: 0.969s, episode steps: 286, steps per second: 295, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46005/100000: episode: 190, duration: 0.659s, episode steps: 189, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46195/100000: episode: 191, duration: 0.654s, episode steps: 190, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46408/100000: episode: 192, duration: 0.730s, episode steps: 213, steps per second: 292, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46578/100000: episode: 193, duration: 0.570s, episode steps: 170, steps per second: 298, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46843/100000: episode: 194, duration: 0.904s, episode steps: 265, steps per second: 293, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47060/100000: episode: 195, duration: 0.746s, episode steps: 217, steps per second: 291, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47301/100000: episode: 196, duration: 0.821s, episode steps: 241, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47486/100000: episode: 197, duration: 0.631s, episode steps: 185, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47668/100000: episode: 198, duration: 0.648s, episode steps: 182, steps per second: 281, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47897/100000: episode: 199, duration: 0.799s, episode steps: 229, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48106/100000: episode: 200, duration: 0.716s, episode steps: 209, steps per second: 292, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48343/100000: episode: 201, duration: 0.803s, episode steps: 237, steps per second: 295, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48631/100000: episode: 202, duration: 0.979s, episode steps: 288, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48867/100000: episode: 203, duration: 0.800s, episode steps: 236, steps per second: 295, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49044/100000: episode: 204, duration: 0.632s, episode steps: 177, steps per second: 280, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.362 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49211/100000: episode: 205, duration: 0.569s, episode steps: 167, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49541/100000: episode: 206, duration: 1.125s, episode steps: 330, steps per second: 293, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49900/100000: episode: 207, duration: 1.218s, episode steps: 359, steps per second: 295, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 50072/100000: episode: 208, duration: 3.894s, episode steps: 172, steps per second:  44, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000960, mae: 0.048676, mean_q: 0.075536, mean_eps: 0.954968\n",
            " 50249/100000: episode: 209, duration: 3.435s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001811, mae: 0.042916, mean_q: 0.068084, mean_eps: 0.954856\n",
            " 50433/100000: episode: 210, duration: 3.514s, episode steps: 184, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002109, mae: 0.042745, mean_q: 0.065920, mean_eps: 0.954692\n",
            " 50623/100000: episode: 211, duration: 3.562s, episode steps: 190, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.001039, mae: 0.041501, mean_q: 0.060473, mean_eps: 0.954525\n",
            " 50913/100000: episode: 212, duration: 5.616s, episode steps: 290, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001543, mae: 0.042010, mean_q: 0.061263, mean_eps: 0.954309\n",
            " 51216/100000: episode: 213, duration: 5.887s, episode steps: 303, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.002776, mae: 0.044254, mean_q: 0.061735, mean_eps: 0.954042\n",
            " 51520/100000: episode: 214, duration: 5.886s, episode steps: 304, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001673, mae: 0.041848, mean_q: 0.058673, mean_eps: 0.953771\n",
            " 51835/100000: episode: 215, duration: 6.006s, episode steps: 315, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.001416, mae: 0.041968, mean_q: 0.059591, mean_eps: 0.953492\n",
            " 52010/100000: episode: 216, duration: 3.370s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.002872, mae: 0.043638, mean_q: 0.058241, mean_eps: 0.953270\n",
            " 52241/100000: episode: 217, duration: 4.469s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.002722, mae: 0.044593, mean_q: 0.063941, mean_eps: 0.953087\n",
            " 52453/100000: episode: 218, duration: 4.053s, episode steps: 212, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.001491, mae: 0.041747, mean_q: 0.057644, mean_eps: 0.952887\n",
            " 52627/100000: episode: 219, duration: 3.283s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001829, mae: 0.042045, mean_q: 0.058224, mean_eps: 0.952714\n",
            " 52942/100000: episode: 220, duration: 6.016s, episode steps: 315, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002790, mae: 0.045301, mean_q: 0.061623, mean_eps: 0.952494\n",
            " 53108/100000: episode: 221, duration: 3.264s, episode steps: 166, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 0.001165, mae: 0.040692, mean_q: 0.057191, mean_eps: 0.952278\n",
            " 53349/100000: episode: 222, duration: 4.637s, episode steps: 241, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002050, mae: 0.041956, mean_q: 0.058682, mean_eps: 0.952095\n",
            " 53522/100000: episode: 223, duration: 3.299s, episode steps: 173, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002207, mae: 0.044340, mean_q: 0.060091, mean_eps: 0.951908\n",
            " 53699/100000: episode: 224, duration: 3.370s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.277 [0.000, 3.000],  loss: 0.001439, mae: 0.042340, mean_q: 0.056791, mean_eps: 0.951751\n",
            " 53870/100000: episode: 225, duration: 3.275s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: 0.004026, mae: 0.046386, mean_q: 0.065230, mean_eps: 0.951594\n",
            " 54047/100000: episode: 226, duration: 3.387s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: 0.001436, mae: 0.041812, mean_q: 0.057063, mean_eps: 0.951438\n",
            " 54255/100000: episode: 227, duration: 3.973s, episode steps: 208, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.002997, mae: 0.043578, mean_q: 0.060095, mean_eps: 0.951265\n",
            " 54431/100000: episode: 228, duration: 3.445s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.002153, mae: 0.044895, mean_q: 0.062674, mean_eps: 0.951092\n",
            " 54669/100000: episode: 229, duration: 4.619s, episode steps: 238, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.391 [0.000, 3.000],  loss: 0.001827, mae: 0.042694, mean_q: 0.058234, mean_eps: 0.950905\n",
            " 54840/100000: episode: 230, duration: 3.308s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: 0.002238, mae: 0.042651, mean_q: 0.059241, mean_eps: 0.950721\n",
            " 55023/100000: episode: 231, duration: 3.580s, episode steps: 183, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 0.004061, mae: 0.048019, mean_q: 0.065578, mean_eps: 0.950563\n",
            " 55257/100000: episode: 232, duration: 4.520s, episode steps: 234, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002112, mae: 0.043315, mean_q: 0.059173, mean_eps: 0.950374\n",
            " 55524/100000: episode: 233, duration: 5.162s, episode steps: 267, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002366, mae: 0.044393, mean_q: 0.062954, mean_eps: 0.950149\n",
            " 55723/100000: episode: 234, duration: 3.823s, episode steps: 199, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.003125, mae: 0.044160, mean_q: 0.059874, mean_eps: 0.949940\n",
            " 55972/100000: episode: 235, duration: 4.794s, episode steps: 249, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.003262, mae: 0.046863, mean_q: 0.063879, mean_eps: 0.949739\n",
            " 56212/100000: episode: 236, duration: 4.707s, episode steps: 240, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001565, mae: 0.041887, mean_q: 0.060068, mean_eps: 0.949519\n",
            " 56527/100000: episode: 237, duration: 6.079s, episode steps: 315, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.001578, mae: 0.042203, mean_q: 0.058174, mean_eps: 0.949269\n",
            " 56738/100000: episode: 238, duration: 4.055s, episode steps: 211, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: 0.001770, mae: 0.043595, mean_q: 0.059320, mean_eps: 0.949031\n",
            " 56923/100000: episode: 239, duration: 3.498s, episode steps: 185, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.001697, mae: 0.041804, mean_q: 0.055736, mean_eps: 0.948853\n",
            " 57263/100000: episode: 240, duration: 6.470s, episode steps: 340, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.002882, mae: 0.044903, mean_q: 0.061091, mean_eps: 0.948617\n",
            " 57583/100000: episode: 241, duration: 6.185s, episode steps: 320, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002468, mae: 0.045028, mean_q: 0.061608, mean_eps: 0.948320\n",
            " 57761/100000: episode: 242, duration: 3.532s, episode steps: 178, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: 0.003326, mae: 0.047001, mean_q: 0.063539, mean_eps: 0.948095\n",
            " 58127/100000: episode: 243, duration: 7.058s, episode steps: 366, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001379, mae: 0.041894, mean_q: 0.057885, mean_eps: 0.947850\n",
            " 58332/100000: episode: 244, duration: 4.006s, episode steps: 205, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.003376, mae: 0.046766, mean_q: 0.062393, mean_eps: 0.947595\n",
            " 58580/100000: episode: 245, duration: 4.806s, episode steps: 248, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001974, mae: 0.044423, mean_q: 0.062862, mean_eps: 0.947391\n",
            " 58847/100000: episode: 246, duration: 5.154s, episode steps: 267, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.001310, mae: 0.042860, mean_q: 0.060405, mean_eps: 0.947159\n",
            " 59078/100000: episode: 247, duration: 4.478s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002364, mae: 0.044810, mean_q: 0.063261, mean_eps: 0.946934\n",
            " 59276/100000: episode: 248, duration: 3.863s, episode steps: 198, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.003790, mae: 0.049806, mean_q: 0.067104, mean_eps: 0.946742\n",
            " 59444/100000: episode: 249, duration: 3.327s, episode steps: 168, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.001831, mae: 0.044188, mean_q: 0.059259, mean_eps: 0.946578\n",
            " 59805/100000: episode: 250, duration: 7.122s, episode steps: 361, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001488, mae: 0.043403, mean_q: 0.059742, mean_eps: 0.946338\n",
            " 60079/100000: episode: 251, duration: 5.222s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000942, mae: 0.043316, mean_q: 0.059375, mean_eps: 0.946052\n",
            " 60456/100000: episode: 252, duration: 7.329s, episode steps: 377, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.002943, mae: 0.050559, mean_q: 0.071978, mean_eps: 0.945761\n",
            " 60626/100000: episode: 253, duration: 3.327s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000521, mae: 0.044906, mean_q: 0.068685, mean_eps: 0.945514\n",
            " 60789/100000: episode: 254, duration: 3.151s, episode steps: 163, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.638 [0.000, 3.000],  loss: 0.003654, mae: 0.055141, mean_q: 0.079850, mean_eps: 0.945363\n",
            " 60987/100000: episode: 255, duration: 3.798s, episode steps: 198, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001577, mae: 0.046268, mean_q: 0.065911, mean_eps: 0.945201\n",
            " 61226/100000: episode: 256, duration: 4.583s, episode steps: 239, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001683, mae: 0.047029, mean_q: 0.065156, mean_eps: 0.945005\n",
            " 61460/100000: episode: 257, duration: 4.547s, episode steps: 234, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.002297, mae: 0.048566, mean_q: 0.066508, mean_eps: 0.944792\n",
            " 61638/100000: episode: 258, duration: 3.468s, episode steps: 178, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001274, mae: 0.046402, mean_q: 0.061930, mean_eps: 0.944607\n",
            " 61810/100000: episode: 259, duration: 3.300s, episode steps: 172, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.002467, mae: 0.049124, mean_q: 0.067569, mean_eps: 0.944448\n",
            " 62134/100000: episode: 260, duration: 6.231s, episode steps: 324, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001667, mae: 0.048249, mean_q: 0.066859, mean_eps: 0.944225\n",
            " 62420/100000: episode: 261, duration: 5.539s, episode steps: 286, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001442, mae: 0.048421, mean_q: 0.069875, mean_eps: 0.943952\n",
            " 62690/100000: episode: 262, duration: 5.256s, episode steps: 270, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002837, mae: 0.049221, mean_q: 0.067571, mean_eps: 0.943701\n",
            " 62876/100000: episode: 263, duration: 3.694s, episode steps: 186, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001642, mae: 0.049869, mean_q: 0.066382, mean_eps: 0.943496\n",
            " 63050/100000: episode: 264, duration: 3.436s, episode steps: 174, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.391 [0.000, 3.000],  loss: 0.001948, mae: 0.048954, mean_q: 0.067186, mean_eps: 0.943334\n",
            " 63266/100000: episode: 265, duration: 4.192s, episode steps: 216, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.000849, mae: 0.044112, mean_q: 0.059925, mean_eps: 0.943158\n",
            " 63544/100000: episode: 266, duration: 5.434s, episode steps: 278, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001421, mae: 0.044735, mean_q: 0.060996, mean_eps: 0.942936\n",
            " 63888/100000: episode: 267, duration: 6.718s, episode steps: 344, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.000878, mae: 0.043780, mean_q: 0.058475, mean_eps: 0.942657\n",
            " 64127/100000: episode: 268, duration: 4.627s, episode steps: 239, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.002377, mae: 0.049121, mean_q: 0.065703, mean_eps: 0.942395\n",
            " 64297/100000: episode: 269, duration: 3.297s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.001607, mae: 0.050942, mean_q: 0.070547, mean_eps: 0.942209\n",
            " 64569/100000: episode: 270, duration: 5.311s, episode steps: 272, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.001830, mae: 0.047766, mean_q: 0.064651, mean_eps: 0.942009\n",
            " 64781/100000: episode: 271, duration: 4.127s, episode steps: 212, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001870, mae: 0.048816, mean_q: 0.066795, mean_eps: 0.941792\n",
            " 65135/100000: episode: 272, duration: 6.835s, episode steps: 354, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.001772, mae: 0.051482, mean_q: 0.071815, mean_eps: 0.941538\n",
            " 65406/100000: episode: 273, duration: 5.214s, episode steps: 271, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001167, mae: 0.048341, mean_q: 0.066229, mean_eps: 0.941257\n",
            " 65592/100000: episode: 274, duration: 3.609s, episode steps: 186, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001465, mae: 0.048432, mean_q: 0.066758, mean_eps: 0.941052\n",
            " 65878/100000: episode: 275, duration: 5.627s, episode steps: 286, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.000757, mae: 0.045817, mean_q: 0.062947, mean_eps: 0.940839\n",
            " 66048/100000: episode: 276, duration: 3.286s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001420, mae: 0.050875, mean_q: 0.068537, mean_eps: 0.940634\n",
            " 66296/100000: episode: 277, duration: 4.885s, episode steps: 248, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002597, mae: 0.050431, mean_q: 0.068861, mean_eps: 0.940447\n",
            " 66512/100000: episode: 278, duration: 4.208s, episode steps: 216, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.001452, mae: 0.047361, mean_q: 0.066149, mean_eps: 0.940238\n",
            " 66786/100000: episode: 279, duration: 5.299s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001530, mae: 0.051765, mean_q: 0.071262, mean_eps: 0.940017\n",
            " 67036/100000: episode: 280, duration: 4.806s, episode steps: 250, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.001059, mae: 0.047169, mean_q: 0.063615, mean_eps: 0.939781\n",
            " 67354/100000: episode: 281, duration: 6.092s, episode steps: 318, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002163, mae: 0.049883, mean_q: 0.068183, mean_eps: 0.939525\n",
            " 67513/100000: episode: 282, duration: 3.051s, episode steps: 159, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001153, mae: 0.046755, mean_q: 0.064490, mean_eps: 0.939309\n",
            " 67759/100000: episode: 283, duration: 4.694s, episode steps: 246, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 0.001807, mae: 0.046878, mean_q: 0.062585, mean_eps: 0.939128\n",
            " 68093/100000: episode: 284, duration: 6.440s, episode steps: 334, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001191, mae: 0.046087, mean_q: 0.062789, mean_eps: 0.938867\n",
            " 68327/100000: episode: 285, duration: 4.475s, episode steps: 234, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001536, mae: 0.045542, mean_q: 0.061535, mean_eps: 0.938611\n",
            " 68598/100000: episode: 286, duration: 5.219s, episode steps: 271, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001405, mae: 0.050705, mean_q: 0.067967, mean_eps: 0.938384\n",
            " 68774/100000: episode: 287, duration: 3.394s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.000821, mae: 0.046673, mean_q: 0.063006, mean_eps: 0.938183\n",
            " 68985/100000: episode: 288, duration: 4.084s, episode steps: 211, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001397, mae: 0.045398, mean_q: 0.061699, mean_eps: 0.938008\n",
            " 69277/100000: episode: 289, duration: 5.625s, episode steps: 292, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.001515, mae: 0.048401, mean_q: 0.066662, mean_eps: 0.937781\n",
            " 69555/100000: episode: 290, duration: 5.289s, episode steps: 278, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.001791, mae: 0.049208, mean_q: 0.067001, mean_eps: 0.937526\n",
            " 69827/100000: episode: 291, duration: 5.301s, episode steps: 272, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000969, mae: 0.047470, mean_q: 0.065062, mean_eps: 0.937279\n",
            " 70191/100000: episode: 292, duration: 6.961s, episode steps: 364, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.002090, mae: 0.051428, mean_q: 0.067893, mean_eps: 0.936993\n",
            " 70500/100000: episode: 293, duration: 5.949s, episode steps: 309, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.002558, mae: 0.054838, mean_q: 0.073530, mean_eps: 0.936690\n",
            " 70761/100000: episode: 294, duration: 5.040s, episode steps: 261, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.360 [0.000, 3.000],  loss: 0.001651, mae: 0.050071, mean_q: 0.068515, mean_eps: 0.936433\n",
            " 70945/100000: episode: 295, duration: 3.543s, episode steps: 184, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000970, mae: 0.052761, mean_q: 0.071633, mean_eps: 0.936231\n",
            " 71185/100000: episode: 296, duration: 4.614s, episode steps: 240, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001902, mae: 0.052782, mean_q: 0.071307, mean_eps: 0.936041\n",
            " 71471/100000: episode: 297, duration: 5.469s, episode steps: 286, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.002945, mae: 0.053885, mean_q: 0.072853, mean_eps: 0.935805\n",
            " 71912/100000: episode: 298, duration: 8.518s, episode steps: 441, steps per second:  52, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001581, mae: 0.051131, mean_q: 0.069854, mean_eps: 0.935479\n",
            " 72363/100000: episode: 299, duration: 8.751s, episode steps: 451, steps per second:  52, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002056, mae: 0.055002, mean_q: 0.074097, mean_eps: 0.935078\n",
            " 72536/100000: episode: 300, duration: 3.392s, episode steps: 173, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.001222, mae: 0.051900, mean_q: 0.071870, mean_eps: 0.934797\n",
            " 72713/100000: episode: 301, duration: 3.426s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001794, mae: 0.051103, mean_q: 0.068500, mean_eps: 0.934638\n",
            " 72921/100000: episode: 302, duration: 4.035s, episode steps: 208, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000820, mae: 0.050821, mean_q: 0.069776, mean_eps: 0.934464\n",
            " 73110/100000: episode: 303, duration: 3.664s, episode steps: 189, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000955, mae: 0.054956, mean_q: 0.074192, mean_eps: 0.934286\n",
            " 73276/100000: episode: 304, duration: 3.303s, episode steps: 166, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.002709, mae: 0.056014, mean_q: 0.076648, mean_eps: 0.934127\n",
            " 73597/100000: episode: 305, duration: 6.227s, episode steps: 321, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001167, mae: 0.055667, mean_q: 0.077800, mean_eps: 0.933908\n",
            " 73833/100000: episode: 306, duration: 4.552s, episode steps: 236, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.001710, mae: 0.053388, mean_q: 0.071352, mean_eps: 0.933656\n",
            " 74071/100000: episode: 307, duration: 4.509s, episode steps: 238, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.000828, mae: 0.049201, mean_q: 0.067151, mean_eps: 0.933443\n",
            " 74303/100000: episode: 308, duration: 4.462s, episode steps: 232, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000983, mae: 0.053149, mean_q: 0.071780, mean_eps: 0.933233\n",
            " 74620/100000: episode: 309, duration: 6.097s, episode steps: 317, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001942, mae: 0.053694, mean_q: 0.072530, mean_eps: 0.932986\n",
            " 74796/100000: episode: 310, duration: 3.470s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.000865, mae: 0.050486, mean_q: 0.070166, mean_eps: 0.932765\n",
            " 75039/100000: episode: 311, duration: 4.737s, episode steps: 243, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001014, mae: 0.051293, mean_q: 0.069775, mean_eps: 0.932576\n",
            " 75417/100000: episode: 312, duration: 7.285s, episode steps: 378, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.000654, mae: 0.050500, mean_q: 0.068242, mean_eps: 0.932295\n",
            " 75652/100000: episode: 313, duration: 4.489s, episode steps: 235, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001423, mae: 0.053564, mean_q: 0.073282, mean_eps: 0.932019\n",
            " 75820/100000: episode: 314, duration: 3.271s, episode steps: 168, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.298 [0.000, 3.000],  loss: 0.000840, mae: 0.053780, mean_q: 0.075419, mean_eps: 0.931839\n",
            " 76016/100000: episode: 315, duration: 3.869s, episode steps: 196, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.001408, mae: 0.051558, mean_q: 0.070230, mean_eps: 0.931676\n",
            " 76192/100000: episode: 316, duration: 3.443s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.000399, mae: 0.045749, mean_q: 0.062693, mean_eps: 0.931508\n",
            " 76422/100000: episode: 317, duration: 4.490s, episode steps: 230, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.339 [0.000, 3.000],  loss: 0.000789, mae: 0.053263, mean_q: 0.072423, mean_eps: 0.931325\n",
            " 76635/100000: episode: 318, duration: 4.066s, episode steps: 213, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.000987, mae: 0.052249, mean_q: 0.070432, mean_eps: 0.931125\n",
            " 76849/100000: episode: 319, duration: 4.150s, episode steps: 214, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000709, mae: 0.052957, mean_q: 0.072875, mean_eps: 0.930932\n",
            " 77028/100000: episode: 320, duration: 3.494s, episode steps: 179, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001337, mae: 0.052857, mean_q: 0.072062, mean_eps: 0.930756\n",
            " 77245/100000: episode: 321, duration: 4.249s, episode steps: 217, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001078, mae: 0.052899, mean_q: 0.071791, mean_eps: 0.930578\n",
            " 77425/100000: episode: 322, duration: 3.477s, episode steps: 180, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.001811, mae: 0.054647, mean_q: 0.073220, mean_eps: 0.930398\n",
            " 77680/100000: episode: 323, duration: 4.902s, episode steps: 255, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.000468, mae: 0.051677, mean_q: 0.069925, mean_eps: 0.930203\n",
            " 77971/100000: episode: 324, duration: 5.574s, episode steps: 291, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.000982, mae: 0.053814, mean_q: 0.074681, mean_eps: 0.929958\n",
            " 78400/100000: episode: 325, duration: 8.352s, episode steps: 429, steps per second:  51, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.000923, mae: 0.050293, mean_q: 0.068177, mean_eps: 0.929634\n",
            " 78809/100000: episode: 326, duration: 7.948s, episode steps: 409, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000694, mae: 0.051957, mean_q: 0.071155, mean_eps: 0.929256\n",
            " 78986/100000: episode: 327, duration: 3.379s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000910, mae: 0.053063, mean_q: 0.073384, mean_eps: 0.928992\n",
            " 79304/100000: episode: 328, duration: 6.152s, episode steps: 318, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.000901, mae: 0.051670, mean_q: 0.069918, mean_eps: 0.928770\n",
            " 79553/100000: episode: 329, duration: 4.818s, episode steps: 249, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.000555, mae: 0.047968, mean_q: 0.066548, mean_eps: 0.928515\n",
            " 79910/100000: episode: 330, duration: 6.845s, episode steps: 357, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.000936, mae: 0.053809, mean_q: 0.072781, mean_eps: 0.928241\n",
            " 80178/100000: episode: 331, duration: 5.179s, episode steps: 268, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.321 [0.000, 3.000],  loss: 0.001380, mae: 0.054082, mean_q: 0.072841, mean_eps: 0.927960\n",
            " 80357/100000: episode: 332, duration: 3.489s, episode steps: 179, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002268, mae: 0.060406, mean_q: 0.082559, mean_eps: 0.927759\n",
            " 80572/100000: episode: 333, duration: 4.187s, episode steps: 215, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.002574, mae: 0.057220, mean_q: 0.079639, mean_eps: 0.927582\n",
            " 80745/100000: episode: 334, duration: 3.396s, episode steps: 173, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001021, mae: 0.054316, mean_q: 0.074252, mean_eps: 0.927408\n",
            " 80913/100000: episode: 335, duration: 3.228s, episode steps: 168, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000993, mae: 0.057444, mean_q: 0.078926, mean_eps: 0.927253\n",
            " 81176/100000: episode: 336, duration: 5.106s, episode steps: 263, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.001315, mae: 0.055927, mean_q: 0.075817, mean_eps: 0.927060\n",
            " 81485/100000: episode: 337, duration: 6.016s, episode steps: 309, steps per second:  51, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001347, mae: 0.054270, mean_q: 0.074092, mean_eps: 0.926803\n",
            " 81850/100000: episode: 338, duration: 7.103s, episode steps: 365, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.001519, mae: 0.057737, mean_q: 0.078364, mean_eps: 0.926499\n",
            " 82331/100000: episode: 339, duration: 9.307s, episode steps: 481, steps per second:  52, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.001168, mae: 0.055396, mean_q: 0.076103, mean_eps: 0.926119\n",
            " 82635/100000: episode: 340, duration: 5.847s, episode steps: 304, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.001288, mae: 0.057209, mean_q: 0.077558, mean_eps: 0.925766\n",
            " 82799/100000: episode: 341, duration: 3.176s, episode steps: 164, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.001137, mae: 0.055243, mean_q: 0.075623, mean_eps: 0.925556\n",
            " 83049/100000: episode: 342, duration: 4.833s, episode steps: 250, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.001108, mae: 0.054935, mean_q: 0.073859, mean_eps: 0.925368\n",
            " 83453/100000: episode: 343, duration: 7.750s, episode steps: 404, steps per second:  52, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.001163, mae: 0.055693, mean_q: 0.076294, mean_eps: 0.925073\n",
            " 83626/100000: episode: 344, duration: 3.342s, episode steps: 173, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.001210, mae: 0.062485, mean_q: 0.084779, mean_eps: 0.924814\n",
            " 83945/100000: episode: 345, duration: 6.227s, episode steps: 319, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000979, mae: 0.053722, mean_q: 0.072506, mean_eps: 0.924593\n",
            " 84191/100000: episode: 346, duration: 4.676s, episode steps: 246, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000985, mae: 0.058489, mean_q: 0.078716, mean_eps: 0.924339\n",
            " 84387/100000: episode: 347, duration: 3.775s, episode steps: 196, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.000887, mae: 0.056510, mean_q: 0.076590, mean_eps: 0.924141\n",
            " 84741/100000: episode: 348, duration: 6.865s, episode steps: 354, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000930, mae: 0.055079, mean_q: 0.075284, mean_eps: 0.923892\n",
            " 84924/100000: episode: 349, duration: 3.590s, episode steps: 183, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000493, mae: 0.050667, mean_q: 0.069635, mean_eps: 0.923651\n",
            " 85214/100000: episode: 350, duration: 5.701s, episode steps: 290, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000633, mae: 0.057782, mean_q: 0.080707, mean_eps: 0.923439\n",
            " 85498/100000: episode: 351, duration: 5.549s, episode steps: 284, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001081, mae: 0.059209, mean_q: 0.081516, mean_eps: 0.923180\n",
            " 85777/100000: episode: 352, duration: 5.426s, episode steps: 279, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000651, mae: 0.056731, mean_q: 0.078491, mean_eps: 0.922926\n",
            " 85989/100000: episode: 353, duration: 4.068s, episode steps: 212, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.000801, mae: 0.055544, mean_q: 0.075792, mean_eps: 0.922704\n",
            " 86380/100000: episode: 354, duration: 7.556s, episode steps: 391, steps per second:  52, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000678, mae: 0.053132, mean_q: 0.071880, mean_eps: 0.922434\n",
            " 86562/100000: episode: 355, duration: 3.530s, episode steps: 182, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.001072, mae: 0.061302, mean_q: 0.083511, mean_eps: 0.922177\n",
            " 86750/100000: episode: 356, duration: 3.620s, episode steps: 188, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.505 [0.000, 3.000],  loss: 0.001238, mae: 0.056725, mean_q: 0.075917, mean_eps: 0.922010\n",
            " 86928/100000: episode: 357, duration: 3.500s, episode steps: 178, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000903, mae: 0.058913, mean_q: 0.080506, mean_eps: 0.921846\n",
            " 87223/100000: episode: 358, duration: 5.816s, episode steps: 295, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.000619, mae: 0.055327, mean_q: 0.076195, mean_eps: 0.921633\n",
            " 87566/100000: episode: 359, duration: 6.719s, episode steps: 343, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.000949, mae: 0.055675, mean_q: 0.077697, mean_eps: 0.921345\n",
            " 87736/100000: episode: 360, duration: 3.321s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000729, mae: 0.056454, mean_q: 0.076551, mean_eps: 0.921115\n",
            " 87903/100000: episode: 361, duration: 3.324s, episode steps: 167, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 0.000406, mae: 0.057380, mean_q: 0.078116, mean_eps: 0.920964\n",
            " 88231/100000: episode: 362, duration: 6.330s, episode steps: 328, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.000806, mae: 0.055521, mean_q: 0.075088, mean_eps: 0.920741\n",
            " 88464/100000: episode: 363, duration: 4.638s, episode steps: 233, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.000268, mae: 0.051149, mean_q: 0.069669, mean_eps: 0.920489\n",
            " 88814/100000: episode: 364, duration: 6.857s, episode steps: 350, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.343 [0.000, 3.000],  loss: 0.000699, mae: 0.054231, mean_q: 0.074194, mean_eps: 0.920226\n",
            " 89054/100000: episode: 365, duration: 4.685s, episode steps: 240, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000448, mae: 0.056421, mean_q: 0.076255, mean_eps: 0.919959\n",
            " 89374/100000: episode: 366, duration: 6.147s, episode steps: 320, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000587, mae: 0.056723, mean_q: 0.075961, mean_eps: 0.919707\n",
            " 89680/100000: episode: 367, duration: 6.004s, episode steps: 306, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001158, mae: 0.057148, mean_q: 0.077132, mean_eps: 0.919427\n",
            " 89851/100000: episode: 368, duration: 3.337s, episode steps: 171, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 0.001153, mae: 0.060396, mean_q: 0.081747, mean_eps: 0.919212\n",
            " 90098/100000: episode: 369, duration: 4.888s, episode steps: 247, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001254, mae: 0.059071, mean_q: 0.078632, mean_eps: 0.919023\n",
            " 90266/100000: episode: 370, duration: 3.302s, episode steps: 168, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002410, mae: 0.060139, mean_q: 0.081670, mean_eps: 0.918836\n",
            " 90496/100000: episode: 371, duration: 4.479s, episode steps: 230, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001848, mae: 0.062535, mean_q: 0.082910, mean_eps: 0.918658\n",
            " 90878/100000: episode: 372, duration: 7.358s, episode steps: 382, steps per second:  52, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002069, mae: 0.064711, mean_q: 0.086410, mean_eps: 0.918383\n",
            " 91109/100000: episode: 373, duration: 4.464s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.001358, mae: 0.062274, mean_q: 0.084004, mean_eps: 0.918105\n",
            " 91286/100000: episode: 374, duration: 3.391s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001898, mae: 0.064895, mean_q: 0.086861, mean_eps: 0.917922\n",
            " 91467/100000: episode: 375, duration: 3.503s, episode steps: 181, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.001506, mae: 0.065083, mean_q: 0.087691, mean_eps: 0.917762\n",
            " 91687/100000: episode: 376, duration: 4.245s, episode steps: 220, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001369, mae: 0.064020, mean_q: 0.086640, mean_eps: 0.917582\n",
            " 91853/100000: episode: 377, duration: 3.227s, episode steps: 166, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.001302, mae: 0.054692, mean_q: 0.074453, mean_eps: 0.917407\n",
            " 92118/100000: episode: 378, duration: 5.104s, episode steps: 265, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000967, mae: 0.057910, mean_q: 0.077976, mean_eps: 0.917213\n",
            " 92297/100000: episode: 379, duration: 3.488s, episode steps: 179, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.492 [0.000, 3.000],  loss: 0.000876, mae: 0.063148, mean_q: 0.085077, mean_eps: 0.917013\n",
            " 92561/100000: episode: 380, duration: 5.099s, episode steps: 264, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001118, mae: 0.067688, mean_q: 0.090821, mean_eps: 0.916813\n",
            " 92738/100000: episode: 381, duration: 3.433s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001096, mae: 0.061116, mean_q: 0.083803, mean_eps: 0.916615\n",
            " 92959/100000: episode: 382, duration: 4.227s, episode steps: 221, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001196, mae: 0.060310, mean_q: 0.081824, mean_eps: 0.916437\n",
            " 93193/100000: episode: 383, duration: 4.582s, episode steps: 234, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000562, mae: 0.057770, mean_q: 0.079190, mean_eps: 0.916232\n",
            " 93433/100000: episode: 384, duration: 4.647s, episode steps: 240, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000461, mae: 0.061190, mean_q: 0.082634, mean_eps: 0.916017\n",
            " 93734/100000: episode: 385, duration: 6.069s, episode steps: 301, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.000998, mae: 0.060465, mean_q: 0.081837, mean_eps: 0.915774\n",
            " 94006/100000: episode: 386, duration: 5.379s, episode steps: 272, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000385, mae: 0.059427, mean_q: 0.080181, mean_eps: 0.915517\n",
            " 94253/100000: episode: 387, duration: 4.892s, episode steps: 247, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000602, mae: 0.055661, mean_q: 0.074522, mean_eps: 0.915283\n",
            " 94478/100000: episode: 388, duration: 4.369s, episode steps: 225, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000653, mae: 0.060948, mean_q: 0.082560, mean_eps: 0.915071\n",
            " 94693/100000: episode: 389, duration: 4.188s, episode steps: 215, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000744, mae: 0.059372, mean_q: 0.080532, mean_eps: 0.914873\n",
            " 94875/100000: episode: 390, duration: 3.491s, episode steps: 182, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000399, mae: 0.062927, mean_q: 0.084631, mean_eps: 0.914694\n",
            " 95182/100000: episode: 391, duration: 5.901s, episode steps: 307, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000436, mae: 0.062340, mean_q: 0.082879, mean_eps: 0.914475\n",
            " 95464/100000: episode: 392, duration: 5.486s, episode steps: 282, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000964, mae: 0.061357, mean_q: 0.083051, mean_eps: 0.914210\n",
            " 95652/100000: episode: 393, duration: 3.748s, episode steps: 188, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000459, mae: 0.060094, mean_q: 0.081419, mean_eps: 0.914000\n",
            " 95877/100000: episode: 394, duration: 4.393s, episode steps: 225, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001271, mae: 0.063833, mean_q: 0.085118, mean_eps: 0.913812\n",
            " 96059/100000: episode: 395, duration: 3.476s, episode steps: 182, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000706, mae: 0.066954, mean_q: 0.090505, mean_eps: 0.913629\n",
            " 96275/100000: episode: 396, duration: 4.166s, episode steps: 216, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001035, mae: 0.060470, mean_q: 0.081157, mean_eps: 0.913451\n",
            " 96504/100000: episode: 397, duration: 4.461s, episode steps: 229, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.000905, mae: 0.058729, mean_q: 0.079366, mean_eps: 0.913251\n",
            " 96742/100000: episode: 398, duration: 4.670s, episode steps: 238, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.001141, mae: 0.061337, mean_q: 0.080879, mean_eps: 0.913040\n",
            " 97048/100000: episode: 399, duration: 6.051s, episode steps: 306, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000621, mae: 0.062006, mean_q: 0.083813, mean_eps: 0.912795\n",
            " 97396/100000: episode: 400, duration: 6.815s, episode steps: 348, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.000636, mae: 0.059957, mean_q: 0.081425, mean_eps: 0.912502\n",
            " 97682/100000: episode: 401, duration: 5.585s, episode steps: 286, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: 0.000689, mae: 0.062887, mean_q: 0.084594, mean_eps: 0.912216\n",
            " 97859/100000: episode: 402, duration: 3.435s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000939, mae: 0.054549, mean_q: 0.073887, mean_eps: 0.912007\n",
            " 98037/100000: episode: 403, duration: 3.503s, episode steps: 178, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.000463, mae: 0.057001, mean_q: 0.078096, mean_eps: 0.911847\n",
            " 98239/100000: episode: 404, duration: 3.927s, episode steps: 202, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000330, mae: 0.055614, mean_q: 0.075406, mean_eps: 0.911676\n",
            " 98491/100000: episode: 405, duration: 4.869s, episode steps: 252, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000687, mae: 0.062311, mean_q: 0.083687, mean_eps: 0.911472\n",
            " 98675/100000: episode: 406, duration: 3.560s, episode steps: 184, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: 0.000621, mae: 0.055710, mean_q: 0.074597, mean_eps: 0.911276\n",
            " 99018/100000: episode: 407, duration: 6.664s, episode steps: 343, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000703, mae: 0.057872, mean_q: 0.077565, mean_eps: 0.911039\n",
            " 99258/100000: episode: 408, duration: 4.632s, episode steps: 240, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000855, mae: 0.060520, mean_q: 0.081515, mean_eps: 0.910776\n",
            " 99472/100000: episode: 409, duration: 4.144s, episode steps: 214, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.000421, mae: 0.055806, mean_q: 0.075413, mean_eps: 0.910572\n",
            " 99706/100000: episode: 410, duration: 4.595s, episode steps: 234, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000361, mae: 0.056429, mean_q: 0.076241, mean_eps: 0.910371\n",
            " 99876/100000: episode: 411, duration: 3.304s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001645, mae: 0.058819, mean_q: 0.080066, mean_eps: 0.910189\n",
            "done, took 1171.917 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW0US9n4LwFl",
        "outputId": "5c59c694-be60-4e31-d86a-7cdafddefaad"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "play.py\n",
        "Script that can display a game played by the agent trained by train.py:\n",
        "* Load the policy network saved in policy.h5\n",
        "* Your agent should use the GreedyQPolicy\n",
        "\"\"\"\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import GreedyQPolicy\n",
        "import tensorflow.keras as K\n",
        "# AtariProcessor = __import__('train').AtariProcessor\n",
        "# create_q_model = __import__('train').create_q_model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # screenshots per state\n",
        "    window = 4  \n",
        "    # Deep Q-Network\n",
        "    model = create_q_model(num_actions, window)  \n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "    \n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
        "                   policy=GreedyQPolicy(),\n",
        "                   processor=processor, memory=memory)\n",
        "    \n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "\n",
        "    # Load the policy network\n",
        "    dqn.load_weights('policy.h5')\n",
        "\n",
        "    # Only works with 'visualize=False' if in Colab\n",
        "    dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 1: reward: 0.000, steps: 170\n",
            "Episode 2: reward: 1.000, steps: 230\n",
            "Episode 3: reward: 3.000, steps: 302\n",
            "Episode 4: reward: 0.000, steps: 171\n",
            "Episode 5: reward: 1.000, steps: 232\n",
            "Episode 6: reward: 1.000, steps: 232\n",
            "Episode 7: reward: 1.000, steps: 10000\n",
            "Episode 8: reward: 2.000, steps: 252\n",
            "Episode 9: reward: 0.000, steps: 159\n",
            "Episode 10: reward: 1.000, steps: 230\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}