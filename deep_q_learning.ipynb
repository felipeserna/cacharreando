{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAU3t26b4fbeSvlKeGZq81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipeserna/cacharreando/blob/master/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12HP8K2VJBMF"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ua8igzLJKNc"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sRiDnEJRQO"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGj_H0SJWwu",
        "outputId": "ab37dd90-de1f-4379-ba53-ffabf5873b90"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f93c6bc08d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85nZ8_g2J7Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afaede3b-8b9e-4775-e525-1903fd6fee7d"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars\n",
        "!pip install keras-rl2\n",
        "!pip install gym\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "Collecting keras-rl2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/fc/143ee05aed804b3b9052d7b17b13832bc7f3c28e7b1bc50edd09c29d8525/keras_rl2-1.0.5-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.31.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2021.5.30)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.4.8)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA-xhVElKGJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "cc1debe1-8513-462e-e4c0-74b7afd374f6"
      },
      "source": [
        "# From stackoverflow for displaying\n",
        "env = gym.make(\"Breakout-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCLPqmxSKnW4",
        "outputId": "f563bd89-4809-49cf-a135-14cf81d8ea2c"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train.py\n",
        "\"\"\"\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras as K\n",
        "from rl.processors import Processor\n",
        "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    \"\"\"\n",
        "    preprocessing\n",
        "    \"\"\"\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\" resizing and grayscale \"\"\"\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize((84, 84), Image.ANTIALIAS).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == (84, 84)\n",
        "        # saves storage in experience memory\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Rescale without using too much memory\n",
        "        \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\" rewards between -1 and 1 \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "\n",
        "def create_q_model(num_actions, window):\n",
        "    \"\"\"\n",
        "    Preprocessing\n",
        "    \"\"\"\n",
        "    # Network\n",
        "\n",
        "    inputs = layers.Input(shape=(window, 84, 84))\n",
        "    # comment the line below to use with GPU\n",
        "    inputs_sort = layers.Permute((2, 3, 1))(inputs)\n",
        "\n",
        "    # Change data_format=\"channels_first\" to use GPU\n",
        "    # change inputs_sort by inputs to use GPU\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(inputs_sort)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return K.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    window = 4\n",
        "    model = create_q_model(num_actions, window)\n",
        "    model.summary()\n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                  value_max=1., value_min=.1, value_test=.05,\n",
        "                                  nb_steps=1000000)\n",
        "\n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions, policy=policy,\n",
        "                   memory=memory, processor=processor,\n",
        "                   nb_steps_warmup=50000, gamma=.99,\n",
        "                   target_model_update=10000,\n",
        "                   train_interval=4,\n",
        "                   delta_clip=1.)\n",
        "\n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "    \n",
        "    dqn.fit(env,\n",
        "            nb_steps=100000,\n",
        "            log_interval=10000,\n",
        "            visualize=False,\n",
        "            verbose=2)\n",
        "\n",
        "    dqn.save_weights('policy.h5', overwrite=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4, 84, 84)]       0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,686,180\n",
            "Trainable params: 1,686,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 100000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   251/100000: episode: 1, duration: 29.944s, episode steps: 251, steps per second:   8, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   475/100000: episode: 2, duration: 0.777s, episode steps: 224, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   724/100000: episode: 3, duration: 0.864s, episode steps: 249, steps per second: 288, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   907/100000: episode: 4, duration: 0.636s, episode steps: 183, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1243/100000: episode: 5, duration: 1.163s, episode steps: 336, steps per second: 289, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1478/100000: episode: 6, duration: 0.816s, episode steps: 235, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  1647/100000: episode: 7, duration: 0.609s, episode steps: 169, steps per second: 278, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2026/100000: episode: 8, duration: 1.309s, episode steps: 379, steps per second: 289, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2311/100000: episode: 9, duration: 0.978s, episode steps: 285, steps per second: 291, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2648/100000: episode: 10, duration: 1.151s, episode steps: 337, steps per second: 293, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  2950/100000: episode: 11, duration: 1.045s, episode steps: 302, steps per second: 289, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3267/100000: episode: 12, duration: 1.104s, episode steps: 317, steps per second: 287, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3640/100000: episode: 13, duration: 1.285s, episode steps: 373, steps per second: 290, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  3851/100000: episode: 14, duration: 0.739s, episode steps: 211, steps per second: 286, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4137/100000: episode: 15, duration: 0.990s, episode steps: 286, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4408/100000: episode: 16, duration: 0.928s, episode steps: 271, steps per second: 292, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4579/100000: episode: 17, duration: 0.611s, episode steps: 171, steps per second: 280, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  4840/100000: episode: 18, duration: 0.903s, episode steps: 261, steps per second: 289, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5074/100000: episode: 19, duration: 0.816s, episode steps: 234, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5240/100000: episode: 20, duration: 0.595s, episode steps: 166, steps per second: 279, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5411/100000: episode: 21, duration: 0.622s, episode steps: 171, steps per second: 275, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.380 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5645/100000: episode: 22, duration: 0.851s, episode steps: 234, steps per second: 275, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  5808/100000: episode: 23, duration: 0.570s, episode steps: 163, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6110/100000: episode: 24, duration: 1.063s, episode steps: 302, steps per second: 284, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6402/100000: episode: 25, duration: 1.010s, episode steps: 292, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6692/100000: episode: 26, duration: 1.000s, episode steps: 290, steps per second: 290, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  6874/100000: episode: 27, duration: 0.639s, episode steps: 182, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7049/100000: episode: 28, duration: 0.598s, episode steps: 175, steps per second: 293, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7325/100000: episode: 29, duration: 0.960s, episode steps: 276, steps per second: 288, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7500/100000: episode: 30, duration: 0.630s, episode steps: 175, steps per second: 278, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7713/100000: episode: 31, duration: 0.748s, episode steps: 213, steps per second: 285, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  7885/100000: episode: 32, duration: 0.600s, episode steps: 172, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.285 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8067/100000: episode: 33, duration: 0.634s, episode steps: 182, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8297/100000: episode: 34, duration: 0.806s, episode steps: 230, steps per second: 285, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8508/100000: episode: 35, duration: 0.743s, episode steps: 211, steps per second: 284, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8834/100000: episode: 36, duration: 1.126s, episode steps: 326, steps per second: 290, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  8997/100000: episode: 37, duration: 0.568s, episode steps: 163, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9381/100000: episode: 38, duration: 1.329s, episode steps: 384, steps per second: 289, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9615/100000: episode: 39, duration: 0.814s, episode steps: 234, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  9847/100000: episode: 40, duration: 0.814s, episode steps: 232, steps per second: 285, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10017/100000: episode: 41, duration: 0.597s, episode steps: 170, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10285/100000: episode: 42, duration: 0.940s, episode steps: 268, steps per second: 285, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10473/100000: episode: 43, duration: 0.658s, episode steps: 188, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.532 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10706/100000: episode: 44, duration: 0.841s, episode steps: 233, steps per second: 277, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 10943/100000: episode: 45, duration: 0.832s, episode steps: 237, steps per second: 285, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11119/100000: episode: 46, duration: 0.621s, episode steps: 176, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11286/100000: episode: 47, duration: 0.582s, episode steps: 167, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11661/100000: episode: 48, duration: 1.287s, episode steps: 375, steps per second: 291, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 11910/100000: episode: 49, duration: 0.870s, episode steps: 249, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12081/100000: episode: 50, duration: 0.602s, episode steps: 171, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12287/100000: episode: 51, duration: 0.712s, episode steps: 206, steps per second: 289, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.335 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12453/100000: episode: 52, duration: 0.572s, episode steps: 166, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12756/100000: episode: 53, duration: 1.049s, episode steps: 303, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 12922/100000: episode: 54, duration: 0.584s, episode steps: 166, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13363/100000: episode: 55, duration: 1.544s, episode steps: 441, steps per second: 286, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13605/100000: episode: 56, duration: 0.868s, episode steps: 242, steps per second: 279, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 13815/100000: episode: 57, duration: 0.742s, episode steps: 210, steps per second: 283, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14068/100000: episode: 58, duration: 0.888s, episode steps: 253, steps per second: 285, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14281/100000: episode: 59, duration: 0.784s, episode steps: 213, steps per second: 272, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14462/100000: episode: 60, duration: 0.635s, episode steps: 181, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14708/100000: episode: 61, duration: 0.873s, episode steps: 246, steps per second: 282, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 14901/100000: episode: 62, duration: 0.678s, episode steps: 193, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15135/100000: episode: 63, duration: 0.802s, episode steps: 234, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15380/100000: episode: 64, duration: 0.862s, episode steps: 245, steps per second: 284, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15598/100000: episode: 65, duration: 0.767s, episode steps: 218, steps per second: 284, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 15865/100000: episode: 66, duration: 0.931s, episode steps: 267, steps per second: 287, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16040/100000: episode: 67, duration: 0.643s, episode steps: 175, steps per second: 272, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16305/100000: episode: 68, duration: 0.951s, episode steps: 265, steps per second: 279, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16543/100000: episode: 69, duration: 0.822s, episode steps: 238, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 16774/100000: episode: 70, duration: 0.803s, episode steps: 231, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17028/100000: episode: 71, duration: 0.888s, episode steps: 254, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17213/100000: episode: 72, duration: 0.644s, episode steps: 185, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17396/100000: episode: 73, duration: 0.640s, episode steps: 183, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17624/100000: episode: 74, duration: 0.791s, episode steps: 228, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 17797/100000: episode: 75, duration: 0.611s, episode steps: 173, steps per second: 283, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18012/100000: episode: 76, duration: 0.750s, episode steps: 215, steps per second: 287, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18318/100000: episode: 77, duration: 1.075s, episode steps: 306, steps per second: 285, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18564/100000: episode: 78, duration: 0.839s, episode steps: 246, steps per second: 293, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 18809/100000: episode: 79, duration: 0.852s, episode steps: 245, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19170/100000: episode: 80, duration: 1.265s, episode steps: 361, steps per second: 285, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.399 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19440/100000: episode: 81, duration: 0.946s, episode steps: 270, steps per second: 285, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19667/100000: episode: 82, duration: 0.790s, episode steps: 227, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 19883/100000: episode: 83, duration: 0.783s, episode steps: 216, steps per second: 276, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20055/100000: episode: 84, duration: 0.606s, episode steps: 172, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.581 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20343/100000: episode: 85, duration: 0.982s, episode steps: 288, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20512/100000: episode: 86, duration: 0.585s, episode steps: 169, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20749/100000: episode: 87, duration: 0.808s, episode steps: 237, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 20968/100000: episode: 88, duration: 0.759s, episode steps: 219, steps per second: 288, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21339/100000: episode: 89, duration: 1.263s, episode steps: 371, steps per second: 294, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21507/100000: episode: 90, duration: 0.587s, episode steps: 168, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 21721/100000: episode: 91, duration: 0.736s, episode steps: 214, steps per second: 291, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22361/100000: episode: 92, duration: 2.198s, episode steps: 640, steps per second: 291, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22589/100000: episode: 93, duration: 0.794s, episode steps: 228, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 22835/100000: episode: 94, duration: 0.859s, episode steps: 246, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23110/100000: episode: 95, duration: 0.936s, episode steps: 275, steps per second: 294, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23397/100000: episode: 96, duration: 0.994s, episode steps: 287, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23620/100000: episode: 97, duration: 0.797s, episode steps: 223, steps per second: 280, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 23907/100000: episode: 98, duration: 1.004s, episode steps: 287, steps per second: 286, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24172/100000: episode: 99, duration: 0.911s, episode steps: 265, steps per second: 291, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24384/100000: episode: 100, duration: 0.730s, episode steps: 212, steps per second: 290, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24609/100000: episode: 101, duration: 0.782s, episode steps: 225, steps per second: 288, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 24902/100000: episode: 102, duration: 1.023s, episode steps: 293, steps per second: 286, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25136/100000: episode: 103, duration: 0.817s, episode steps: 234, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25379/100000: episode: 104, duration: 0.857s, episode steps: 243, steps per second: 284, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 25702/100000: episode: 105, duration: 1.115s, episode steps: 323, steps per second: 290, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26007/100000: episode: 106, duration: 1.058s, episode steps: 305, steps per second: 288, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26199/100000: episode: 107, duration: 0.667s, episode steps: 192, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26376/100000: episode: 108, duration: 0.618s, episode steps: 177, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26587/100000: episode: 109, duration: 0.736s, episode steps: 211, steps per second: 287, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 26767/100000: episode: 110, duration: 0.644s, episode steps: 180, steps per second: 280, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27074/100000: episode: 111, duration: 1.061s, episode steps: 307, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27352/100000: episode: 112, duration: 0.961s, episode steps: 278, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27568/100000: episode: 113, duration: 0.772s, episode steps: 216, steps per second: 280, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27767/100000: episode: 114, duration: 0.694s, episode steps: 199, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 27998/100000: episode: 115, duration: 0.828s, episode steps: 231, steps per second: 279, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28234/100000: episode: 116, duration: 0.826s, episode steps: 236, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28481/100000: episode: 117, duration: 0.850s, episode steps: 247, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28657/100000: episode: 118, duration: 0.619s, episode steps: 176, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 28871/100000: episode: 119, duration: 0.756s, episode steps: 214, steps per second: 283, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29139/100000: episode: 120, duration: 0.921s, episode steps: 268, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29499/100000: episode: 121, duration: 1.240s, episode steps: 360, steps per second: 290, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 29750/100000: episode: 122, duration: 0.863s, episode steps: 251, steps per second: 291, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30096/100000: episode: 123, duration: 1.192s, episode steps: 346, steps per second: 290, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30482/100000: episode: 124, duration: 1.336s, episode steps: 386, steps per second: 289, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30648/100000: episode: 125, duration: 0.600s, episode steps: 166, steps per second: 277, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30819/100000: episode: 126, duration: 0.599s, episode steps: 171, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 30989/100000: episode: 127, duration: 0.615s, episode steps: 170, steps per second: 276, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31204/100000: episode: 128, duration: 0.751s, episode steps: 215, steps per second: 286, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31419/100000: episode: 129, duration: 0.750s, episode steps: 215, steps per second: 287, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31602/100000: episode: 130, duration: 0.656s, episode steps: 183, steps per second: 279, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 31827/100000: episode: 131, duration: 0.797s, episode steps: 225, steps per second: 282, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32040/100000: episode: 132, duration: 0.758s, episode steps: 213, steps per second: 281, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32280/100000: episode: 133, duration: 0.837s, episode steps: 240, steps per second: 287, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32714/100000: episode: 134, duration: 1.489s, episode steps: 434, steps per second: 291, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 32883/100000: episode: 135, duration: 0.593s, episode steps: 169, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33165/100000: episode: 136, duration: 0.978s, episode steps: 282, steps per second: 288, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33370/100000: episode: 137, duration: 0.721s, episode steps: 205, steps per second: 284, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33585/100000: episode: 138, duration: 0.741s, episode steps: 215, steps per second: 290, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33789/100000: episode: 139, duration: 0.722s, episode steps: 204, steps per second: 283, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 33976/100000: episode: 140, duration: 0.658s, episode steps: 187, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34205/100000: episode: 141, duration: 0.802s, episode steps: 229, steps per second: 285, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34506/100000: episode: 142, duration: 1.050s, episode steps: 301, steps per second: 287, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34691/100000: episode: 143, duration: 0.654s, episode steps: 185, steps per second: 283, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 34864/100000: episode: 144, duration: 0.630s, episode steps: 173, steps per second: 275, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35099/100000: episode: 145, duration: 0.805s, episode steps: 235, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35273/100000: episode: 146, duration: 0.606s, episode steps: 174, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35508/100000: episode: 147, duration: 0.820s, episode steps: 235, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35684/100000: episode: 148, duration: 0.607s, episode steps: 176, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 35928/100000: episode: 149, duration: 0.835s, episode steps: 244, steps per second: 292, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36172/100000: episode: 150, duration: 0.834s, episode steps: 244, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36341/100000: episode: 151, duration: 0.632s, episode steps: 169, steps per second: 267, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36514/100000: episode: 152, duration: 0.607s, episode steps: 173, steps per second: 285, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 36751/100000: episode: 153, duration: 0.820s, episode steps: 237, steps per second: 289, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37011/100000: episode: 154, duration: 0.886s, episode steps: 260, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37190/100000: episode: 155, duration: 0.620s, episode steps: 179, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37363/100000: episode: 156, duration: 0.600s, episode steps: 173, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.428 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37607/100000: episode: 157, duration: 0.838s, episode steps: 244, steps per second: 291, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 37788/100000: episode: 158, duration: 0.632s, episode steps: 181, steps per second: 286, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38011/100000: episode: 159, duration: 0.762s, episode steps: 223, steps per second: 293, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38194/100000: episode: 160, duration: 0.639s, episode steps: 183, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 38541/100000: episode: 161, duration: 1.187s, episode steps: 347, steps per second: 292, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39042/100000: episode: 162, duration: 1.720s, episode steps: 501, steps per second: 291, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39355/100000: episode: 163, duration: 1.089s, episode steps: 313, steps per second: 287, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 39859/100000: episode: 164, duration: 1.724s, episode steps: 504, steps per second: 292, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40065/100000: episode: 165, duration: 0.721s, episode steps: 206, steps per second: 286, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40412/100000: episode: 166, duration: 1.194s, episode steps: 347, steps per second: 291, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40647/100000: episode: 167, duration: 0.799s, episode steps: 235, steps per second: 294, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 40820/100000: episode: 168, duration: 0.604s, episode steps: 173, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41074/100000: episode: 169, duration: 0.881s, episode steps: 254, steps per second: 288, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41376/100000: episode: 170, duration: 1.045s, episode steps: 302, steps per second: 289, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41599/100000: episode: 171, duration: 0.769s, episode steps: 223, steps per second: 290, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41771/100000: episode: 172, duration: 0.607s, episode steps: 172, steps per second: 284, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 41997/100000: episode: 173, duration: 0.791s, episode steps: 226, steps per second: 286, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42269/100000: episode: 174, duration: 0.942s, episode steps: 272, steps per second: 289, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42440/100000: episode: 175, duration: 0.594s, episode steps: 171, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42676/100000: episode: 176, duration: 0.849s, episode steps: 236, steps per second: 278, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 42844/100000: episode: 177, duration: 0.583s, episode steps: 168, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43269/100000: episode: 178, duration: 1.436s, episode steps: 425, steps per second: 296, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 43568/100000: episode: 179, duration: 1.029s, episode steps: 299, steps per second: 291, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44029/100000: episode: 180, duration: 1.577s, episode steps: 461, steps per second: 292, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44225/100000: episode: 181, duration: 0.693s, episode steps: 196, steps per second: 283, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44483/100000: episode: 182, duration: 0.901s, episode steps: 258, steps per second: 286, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44778/100000: episode: 183, duration: 1.012s, episode steps: 295, steps per second: 292, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 44949/100000: episode: 184, duration: 0.593s, episode steps: 171, steps per second: 288, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45258/100000: episode: 185, duration: 1.076s, episode steps: 309, steps per second: 287, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45527/100000: episode: 186, duration: 0.919s, episode steps: 269, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45705/100000: episode: 187, duration: 0.613s, episode steps: 178, steps per second: 290, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.354 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 45877/100000: episode: 188, duration: 0.599s, episode steps: 172, steps per second: 287, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46305/100000: episode: 189, duration: 1.464s, episode steps: 428, steps per second: 292, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46484/100000: episode: 190, duration: 0.619s, episode steps: 179, steps per second: 289, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 46834/100000: episode: 191, duration: 1.256s, episode steps: 350, steps per second: 279, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47078/100000: episode: 192, duration: 0.873s, episode steps: 244, steps per second: 280, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47252/100000: episode: 193, duration: 0.616s, episode steps: 174, steps per second: 282, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47511/100000: episode: 194, duration: 0.913s, episode steps: 259, steps per second: 284, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47739/100000: episode: 195, duration: 0.812s, episode steps: 228, steps per second: 281, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 47913/100000: episode: 196, duration: 0.621s, episode steps: 174, steps per second: 280, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48139/100000: episode: 197, duration: 0.803s, episode steps: 226, steps per second: 281, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48492/100000: episode: 198, duration: 1.232s, episode steps: 353, steps per second: 287, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 48763/100000: episode: 199, duration: 0.956s, episode steps: 271, steps per second: 284, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49117/100000: episode: 200, duration: 1.210s, episode steps: 354, steps per second: 292, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49329/100000: episode: 201, duration: 0.743s, episode steps: 212, steps per second: 285, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49626/100000: episode: 202, duration: 1.013s, episode steps: 297, steps per second: 293, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 49915/100000: episode: 203, duration: 0.993s, episode steps: 289, steps per second: 291, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 50139/100000: episode: 204, duration: 5.157s, episode steps: 224, steps per second:  43, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.004675, mae: 0.089143, mean_q: 0.135191, mean_eps: 0.954937\n",
            " 50305/100000: episode: 205, duration: 3.365s, episode steps: 166, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.003622, mae: 0.085206, mean_q: 0.122109, mean_eps: 0.954800\n",
            " 50599/100000: episode: 206, duration: 5.825s, episode steps: 294, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.002908, mae: 0.083298, mean_q: 0.118661, mean_eps: 0.954593\n",
            " 50770/100000: episode: 207, duration: 3.387s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.002263, mae: 0.082979, mean_q: 0.119124, mean_eps: 0.954384\n",
            " 50948/100000: episode: 208, duration: 3.569s, episode steps: 178, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.725 [0.000, 3.000],  loss: 0.001868, mae: 0.080236, mean_q: 0.114354, mean_eps: 0.954228\n",
            " 51289/100000: episode: 209, duration: 6.893s, episode steps: 341, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.002633, mae: 0.083164, mean_q: 0.117038, mean_eps: 0.953994\n",
            " 51453/100000: episode: 210, duration: 3.279s, episode steps: 164, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.002779, mae: 0.082401, mean_q: 0.118394, mean_eps: 0.953765\n",
            " 51680/100000: episode: 211, duration: 4.647s, episode steps: 227, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.002907, mae: 0.083197, mean_q: 0.116838, mean_eps: 0.953591\n",
            " 51950/100000: episode: 212, duration: 5.457s, episode steps: 270, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.003267, mae: 0.085502, mean_q: 0.121616, mean_eps: 0.953367\n",
            " 52129/100000: episode: 213, duration: 3.637s, episode steps: 179, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001816, mae: 0.080355, mean_q: 0.113577, mean_eps: 0.953164\n",
            " 52303/100000: episode: 214, duration: 3.418s, episode steps: 174, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.003680, mae: 0.085344, mean_q: 0.119869, mean_eps: 0.953006\n",
            " 52651/100000: episode: 215, duration: 7.014s, episode steps: 348, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.003110, mae: 0.084037, mean_q: 0.117747, mean_eps: 0.952772\n",
            " 52920/100000: episode: 216, duration: 5.410s, episode steps: 269, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003088, mae: 0.084289, mean_q: 0.116543, mean_eps: 0.952494\n",
            " 53168/100000: episode: 217, duration: 4.905s, episode steps: 248, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002828, mae: 0.084187, mean_q: 0.116693, mean_eps: 0.952262\n",
            " 53386/100000: episode: 218, duration: 4.271s, episode steps: 218, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.003488, mae: 0.084210, mean_q: 0.113656, mean_eps: 0.952052\n",
            " 53604/100000: episode: 219, duration: 4.278s, episode steps: 218, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.002971, mae: 0.084900, mean_q: 0.116124, mean_eps: 0.951855\n",
            " 53958/100000: episode: 220, duration: 6.939s, episode steps: 354, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.003025, mae: 0.082334, mean_q: 0.111962, mean_eps: 0.951598\n",
            " 54167/100000: episode: 221, duration: 4.120s, episode steps: 209, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001567, mae: 0.081286, mean_q: 0.110884, mean_eps: 0.951344\n",
            " 54414/100000: episode: 222, duration: 4.886s, episode steps: 247, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.002569, mae: 0.083358, mean_q: 0.113854, mean_eps: 0.951139\n",
            " 54751/100000: episode: 223, duration: 6.549s, episode steps: 337, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.002461, mae: 0.081715, mean_q: 0.112021, mean_eps: 0.950876\n",
            " 54939/100000: episode: 224, duration: 3.706s, episode steps: 188, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.002065, mae: 0.082178, mean_q: 0.113460, mean_eps: 0.950640\n",
            " 55206/100000: episode: 225, duration: 5.241s, episode steps: 267, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.001431, mae: 0.080035, mean_q: 0.109478, mean_eps: 0.950435\n",
            " 55427/100000: episode: 226, duration: 4.339s, episode steps: 221, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.004017, mae: 0.086444, mean_q: 0.119546, mean_eps: 0.950216\n",
            " 55667/100000: episode: 227, duration: 4.791s, episode steps: 240, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002871, mae: 0.082808, mean_q: 0.111352, mean_eps: 0.950009\n",
            " 55848/100000: episode: 228, duration: 3.628s, episode steps: 181, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.004465, mae: 0.087026, mean_q: 0.117689, mean_eps: 0.949820\n",
            " 56040/100000: episode: 229, duration: 3.821s, episode steps: 192, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.380 [0.000, 3.000],  loss: 0.002262, mae: 0.083400, mean_q: 0.114239, mean_eps: 0.949652\n",
            " 56275/100000: episode: 230, duration: 4.627s, episode steps: 235, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002388, mae: 0.082474, mean_q: 0.113838, mean_eps: 0.949460\n",
            " 56595/100000: episode: 231, duration: 6.271s, episode steps: 320, steps per second:  51, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.002790, mae: 0.083677, mean_q: 0.113536, mean_eps: 0.949209\n",
            " 56890/100000: episode: 232, duration: 5.846s, episode steps: 295, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.342 [0.000, 3.000],  loss: 0.002962, mae: 0.084535, mean_q: 0.117199, mean_eps: 0.948932\n",
            " 57181/100000: episode: 233, duration: 5.736s, episode steps: 291, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.002598, mae: 0.084496, mean_q: 0.114019, mean_eps: 0.948668\n",
            " 57469/100000: episode: 234, duration: 5.667s, episode steps: 288, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.002971, mae: 0.086010, mean_q: 0.116697, mean_eps: 0.948407\n",
            " 57725/100000: episode: 235, duration: 4.987s, episode steps: 256, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002049, mae: 0.082980, mean_q: 0.114341, mean_eps: 0.948162\n",
            " 57949/100000: episode: 236, duration: 4.374s, episode steps: 224, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.001841, mae: 0.084661, mean_q: 0.116734, mean_eps: 0.947946\n",
            " 58220/100000: episode: 237, duration: 5.319s, episode steps: 271, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.001753, mae: 0.080868, mean_q: 0.110244, mean_eps: 0.947724\n",
            " 58480/100000: episode: 238, duration: 5.170s, episode steps: 260, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.002245, mae: 0.083067, mean_q: 0.111485, mean_eps: 0.947487\n",
            " 58859/100000: episode: 239, duration: 7.614s, episode steps: 379, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.001678, mae: 0.082672, mean_q: 0.112129, mean_eps: 0.947199\n",
            " 59098/100000: episode: 240, duration: 4.731s, episode steps: 239, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001770, mae: 0.084684, mean_q: 0.116584, mean_eps: 0.946920\n",
            " 59366/100000: episode: 241, duration: 5.268s, episode steps: 268, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.002377, mae: 0.085051, mean_q: 0.116562, mean_eps: 0.946691\n",
            " 59556/100000: episode: 242, duration: 3.778s, episode steps: 190, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.003383, mae: 0.085505, mean_q: 0.114531, mean_eps: 0.946486\n",
            " 59802/100000: episode: 243, duration: 4.809s, episode steps: 246, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001916, mae: 0.082695, mean_q: 0.110919, mean_eps: 0.946290\n",
            " 59991/100000: episode: 244, duration: 3.660s, episode steps: 189, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001915, mae: 0.082204, mean_q: 0.109075, mean_eps: 0.946094\n",
            " 60339/100000: episode: 245, duration: 6.788s, episode steps: 348, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.001750, mae: 0.087235, mean_q: 0.118718, mean_eps: 0.945852\n",
            " 60524/100000: episode: 246, duration: 3.647s, episode steps: 185, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 0.002181, mae: 0.089324, mean_q: 0.121061, mean_eps: 0.945613\n",
            " 61108/100000: episode: 247, duration: 11.414s, episode steps: 584, steps per second:  51, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002016, mae: 0.088790, mean_q: 0.119657, mean_eps: 0.945267\n",
            " 61319/100000: episode: 248, duration: 4.120s, episode steps: 211, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.001552, mae: 0.085846, mean_q: 0.115763, mean_eps: 0.944909\n",
            " 61702/100000: episode: 249, duration: 7.493s, episode steps: 383, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001518, mae: 0.089121, mean_q: 0.122307, mean_eps: 0.944641\n",
            " 61912/100000: episode: 250, duration: 4.127s, episode steps: 210, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001697, mae: 0.088853, mean_q: 0.122370, mean_eps: 0.944375\n",
            " 62189/100000: episode: 251, duration: 5.577s, episode steps: 277, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001268, mae: 0.089115, mean_q: 0.120445, mean_eps: 0.944155\n",
            " 62358/100000: episode: 252, duration: 3.309s, episode steps: 169, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002378, mae: 0.094131, mean_q: 0.126355, mean_eps: 0.943953\n",
            " 62566/100000: episode: 253, duration: 4.077s, episode steps: 208, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003396, mae: 0.091011, mean_q: 0.121460, mean_eps: 0.943784\n",
            " 62802/100000: episode: 254, duration: 4.595s, episode steps: 236, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001501, mae: 0.089345, mean_q: 0.122030, mean_eps: 0.943584\n",
            " 62974/100000: episode: 255, duration: 3.358s, episode steps: 172, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002716, mae: 0.092133, mean_q: 0.123449, mean_eps: 0.943401\n",
            " 63231/100000: episode: 256, duration: 5.042s, episode steps: 257, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.001687, mae: 0.089250, mean_q: 0.123803, mean_eps: 0.943208\n",
            " 63594/100000: episode: 257, duration: 7.185s, episode steps: 363, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.001715, mae: 0.089500, mean_q: 0.121027, mean_eps: 0.942929\n",
            " 63840/100000: episode: 258, duration: 4.917s, episode steps: 246, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.001543, mae: 0.087506, mean_q: 0.120019, mean_eps: 0.942656\n",
            " 64021/100000: episode: 259, duration: 3.595s, episode steps: 181, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002418, mae: 0.092185, mean_q: 0.125709, mean_eps: 0.942463\n",
            " 64342/100000: episode: 260, duration: 6.297s, episode steps: 321, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.002635, mae: 0.088946, mean_q: 0.120778, mean_eps: 0.942236\n",
            " 64613/100000: episode: 261, duration: 5.287s, episode steps: 271, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.002379, mae: 0.092414, mean_q: 0.127112, mean_eps: 0.941970\n",
            " 64819/100000: episode: 262, duration: 4.005s, episode steps: 206, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001480, mae: 0.088598, mean_q: 0.123455, mean_eps: 0.941756\n",
            " 65224/100000: episode: 263, duration: 7.980s, episode steps: 405, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.002417, mae: 0.091903, mean_q: 0.124334, mean_eps: 0.941482\n",
            " 65392/100000: episode: 264, duration: 3.337s, episode steps: 168, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002537, mae: 0.091078, mean_q: 0.124661, mean_eps: 0.941225\n",
            " 65667/100000: episode: 265, duration: 5.414s, episode steps: 275, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001210, mae: 0.087967, mean_q: 0.118543, mean_eps: 0.941025\n",
            " 65836/100000: episode: 266, duration: 3.388s, episode steps: 169, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.001274, mae: 0.085747, mean_q: 0.115624, mean_eps: 0.940825\n",
            " 66258/100000: episode: 267, duration: 8.257s, episode steps: 422, steps per second:  51, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001699, mae: 0.088491, mean_q: 0.119065, mean_eps: 0.940559\n",
            " 66683/100000: episode: 268, duration: 8.302s, episode steps: 425, steps per second:  51, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.001389, mae: 0.088605, mean_q: 0.119140, mean_eps: 0.940177\n",
            " 66849/100000: episode: 269, duration: 3.330s, episode steps: 166, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001250, mae: 0.089067, mean_q: 0.121875, mean_eps: 0.939911\n",
            " 67309/100000: episode: 270, duration: 9.208s, episode steps: 460, steps per second:  50, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001755, mae: 0.090134, mean_q: 0.122562, mean_eps: 0.939628\n",
            " 67522/100000: episode: 271, duration: 4.229s, episode steps: 213, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.001086, mae: 0.088735, mean_q: 0.119525, mean_eps: 0.939326\n",
            " 67699/100000: episode: 272, duration: 3.493s, episode steps: 177, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.000733, mae: 0.084628, mean_q: 0.113750, mean_eps: 0.939151\n",
            " 67970/100000: episode: 273, duration: 5.306s, episode steps: 271, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001034, mae: 0.085783, mean_q: 0.115874, mean_eps: 0.938949\n",
            " 68393/100000: episode: 274, duration: 8.220s, episode steps: 423, steps per second:  51, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001043, mae: 0.086033, mean_q: 0.116579, mean_eps: 0.938636\n",
            " 68687/100000: episode: 275, duration: 5.696s, episode steps: 294, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.002281, mae: 0.090227, mean_q: 0.121933, mean_eps: 0.938314\n",
            " 68898/100000: episode: 276, duration: 4.161s, episode steps: 211, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001318, mae: 0.087397, mean_q: 0.117722, mean_eps: 0.938087\n",
            " 69232/100000: episode: 277, duration: 6.533s, episode steps: 334, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.001323, mae: 0.088223, mean_q: 0.120049, mean_eps: 0.937842\n",
            " 69539/100000: episode: 278, duration: 6.015s, episode steps: 307, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001268, mae: 0.087453, mean_q: 0.117097, mean_eps: 0.937554\n",
            " 69709/100000: episode: 279, duration: 3.345s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.347 [0.000, 3.000],  loss: 0.000343, mae: 0.083930, mean_q: 0.112452, mean_eps: 0.937338\n",
            " 69965/100000: episode: 280, duration: 5.013s, episode steps: 256, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001030, mae: 0.085337, mean_q: 0.114735, mean_eps: 0.937146\n",
            " 70182/100000: episode: 281, duration: 4.221s, episode steps: 217, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.002899, mae: 0.101111, mean_q: 0.134254, mean_eps: 0.936933\n",
            " 70426/100000: episode: 282, duration: 4.798s, episode steps: 244, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001907, mae: 0.099910, mean_q: 0.135904, mean_eps: 0.936726\n",
            " 70678/100000: episode: 283, duration: 4.957s, episode steps: 252, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.001064, mae: 0.095586, mean_q: 0.128329, mean_eps: 0.936503\n",
            " 70847/100000: episode: 284, duration: 3.333s, episode steps: 169, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: 0.002589, mae: 0.099328, mean_q: 0.133767, mean_eps: 0.936314\n",
            " 71126/100000: episode: 285, duration: 5.565s, episode steps: 279, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001955, mae: 0.098764, mean_q: 0.134822, mean_eps: 0.936113\n",
            " 71458/100000: episode: 286, duration: 6.560s, episode steps: 332, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001289, mae: 0.095750, mean_q: 0.129478, mean_eps: 0.935837\n",
            " 71761/100000: episode: 287, duration: 5.993s, episode steps: 303, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.001947, mae: 0.100259, mean_q: 0.135516, mean_eps: 0.935551\n",
            " 72018/100000: episode: 288, duration: 5.029s, episode steps: 257, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001126, mae: 0.097462, mean_q: 0.131822, mean_eps: 0.935299\n",
            " 72314/100000: episode: 289, duration: 5.746s, episode steps: 296, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001723, mae: 0.097563, mean_q: 0.131682, mean_eps: 0.935051\n",
            " 72663/100000: episode: 290, duration: 6.846s, episode steps: 349, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001465, mae: 0.099345, mean_q: 0.134125, mean_eps: 0.934761\n",
            " 72915/100000: episode: 291, duration: 4.941s, episode steps: 252, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001417, mae: 0.098233, mean_q: 0.131223, mean_eps: 0.934491\n",
            " 73089/100000: episode: 292, duration: 3.444s, episode steps: 174, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.001240, mae: 0.097276, mean_q: 0.130777, mean_eps: 0.934298\n",
            " 73457/100000: episode: 293, duration: 7.206s, episode steps: 368, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.001612, mae: 0.096879, mean_q: 0.130462, mean_eps: 0.934053\n",
            " 73726/100000: episode: 294, duration: 5.278s, episode steps: 269, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.331 [0.000, 3.000],  loss: 0.001617, mae: 0.098674, mean_q: 0.133550, mean_eps: 0.933767\n",
            " 73961/100000: episode: 295, duration: 4.734s, episode steps: 235, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.001254, mae: 0.095936, mean_q: 0.129044, mean_eps: 0.933540\n",
            " 74185/100000: episode: 296, duration: 4.414s, episode steps: 224, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002024, mae: 0.102864, mean_q: 0.138026, mean_eps: 0.933333\n",
            " 74540/100000: episode: 297, duration: 6.942s, episode steps: 355, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.001429, mae: 0.098265, mean_q: 0.131101, mean_eps: 0.933074\n",
            " 74769/100000: episode: 298, duration: 4.512s, episode steps: 229, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000822, mae: 0.094655, mean_q: 0.126577, mean_eps: 0.932811\n",
            " 75009/100000: episode: 299, duration: 4.772s, episode steps: 240, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.001003, mae: 0.095097, mean_q: 0.128183, mean_eps: 0.932599\n",
            " 75235/100000: episode: 300, duration: 4.378s, episode steps: 226, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.000855, mae: 0.096476, mean_q: 0.130694, mean_eps: 0.932390\n",
            " 75539/100000: episode: 301, duration: 6.007s, episode steps: 304, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.002038, mae: 0.098996, mean_q: 0.133356, mean_eps: 0.932153\n",
            " 75823/100000: episode: 302, duration: 5.666s, episode steps: 284, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000504, mae: 0.095195, mean_q: 0.129157, mean_eps: 0.931888\n",
            " 76058/100000: episode: 303, duration: 4.687s, episode steps: 235, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001072, mae: 0.095622, mean_q: 0.128517, mean_eps: 0.931654\n",
            " 76364/100000: episode: 304, duration: 6.039s, episode steps: 306, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000874, mae: 0.096986, mean_q: 0.131202, mean_eps: 0.931411\n",
            " 76530/100000: episode: 305, duration: 3.336s, episode steps: 166, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002300, mae: 0.095036, mean_q: 0.128599, mean_eps: 0.931199\n",
            " 76763/100000: episode: 306, duration: 4.551s, episode steps: 233, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.001908, mae: 0.100419, mean_q: 0.136929, mean_eps: 0.931019\n",
            " 77094/100000: episode: 307, duration: 6.719s, episode steps: 331, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.002849, mae: 0.101281, mean_q: 0.134569, mean_eps: 0.930765\n",
            " 77301/100000: episode: 308, duration: 4.103s, episode steps: 207, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000648, mae: 0.093544, mean_q: 0.125532, mean_eps: 0.930522\n",
            " 77582/100000: episode: 309, duration: 5.509s, episode steps: 281, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001162, mae: 0.097290, mean_q: 0.130031, mean_eps: 0.930302\n",
            " 77842/100000: episode: 310, duration: 5.064s, episode steps: 260, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.265 [0.000, 3.000],  loss: 0.001150, mae: 0.093877, mean_q: 0.128621, mean_eps: 0.930059\n",
            " 78020/100000: episode: 311, duration: 3.552s, episode steps: 178, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001365, mae: 0.096989, mean_q: 0.130851, mean_eps: 0.929863\n",
            " 78215/100000: episode: 312, duration: 3.837s, episode steps: 195, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.395 [0.000, 3.000],  loss: 0.001328, mae: 0.098452, mean_q: 0.132713, mean_eps: 0.929696\n",
            " 78391/100000: episode: 313, duration: 3.424s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.369 [0.000, 3.000],  loss: 0.001180, mae: 0.096481, mean_q: 0.129371, mean_eps: 0.929528\n",
            " 78620/100000: episode: 314, duration: 4.561s, episode steps: 229, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 0.001821, mae: 0.103032, mean_q: 0.139750, mean_eps: 0.929346\n",
            " 78800/100000: episode: 315, duration: 3.640s, episode steps: 180, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002251, mae: 0.102604, mean_q: 0.138821, mean_eps: 0.929163\n",
            " 79109/100000: episode: 316, duration: 6.240s, episode steps: 309, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001323, mae: 0.098651, mean_q: 0.133538, mean_eps: 0.928941\n",
            " 79279/100000: episode: 317, duration: 3.318s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001478, mae: 0.096759, mean_q: 0.130199, mean_eps: 0.928725\n",
            " 79572/100000: episode: 318, duration: 5.829s, episode steps: 293, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.399 [0.000, 3.000],  loss: 0.001210, mae: 0.097950, mean_q: 0.132249, mean_eps: 0.928518\n",
            " 79752/100000: episode: 319, duration: 3.641s, episode steps: 180, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000947, mae: 0.100714, mean_q: 0.136330, mean_eps: 0.928306\n",
            " 79996/100000: episode: 320, duration: 4.910s, episode steps: 244, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001159, mae: 0.096681, mean_q: 0.129419, mean_eps: 0.928115\n",
            " 80166/100000: episode: 321, duration: 3.403s, episode steps: 170, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.001496, mae: 0.102418, mean_q: 0.137231, mean_eps: 0.927928\n",
            " 80394/100000: episode: 322, duration: 4.518s, episode steps: 228, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001646, mae: 0.107249, mean_q: 0.146172, mean_eps: 0.927748\n",
            " 80629/100000: episode: 323, duration: 4.643s, episode steps: 235, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.001174, mae: 0.102552, mean_q: 0.138917, mean_eps: 0.927539\n",
            " 80949/100000: episode: 324, duration: 6.302s, episode steps: 320, steps per second:  51, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.001161, mae: 0.102409, mean_q: 0.137293, mean_eps: 0.927289\n",
            " 81195/100000: episode: 325, duration: 4.802s, episode steps: 246, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.000732, mae: 0.105369, mean_q: 0.143157, mean_eps: 0.927035\n",
            " 81383/100000: episode: 326, duration: 3.712s, episode steps: 188, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000589, mae: 0.102681, mean_q: 0.138762, mean_eps: 0.926841\n",
            " 81566/100000: episode: 327, duration: 3.692s, episode steps: 183, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002879, mae: 0.107478, mean_q: 0.142637, mean_eps: 0.926673\n",
            " 81741/100000: episode: 328, duration: 3.455s, episode steps: 175, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.001791, mae: 0.106091, mean_q: 0.143526, mean_eps: 0.926511\n",
            " 81973/100000: episode: 329, duration: 4.625s, episode steps: 232, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000721, mae: 0.099489, mean_q: 0.134014, mean_eps: 0.926328\n",
            " 82248/100000: episode: 330, duration: 5.485s, episode steps: 275, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001157, mae: 0.103929, mean_q: 0.141938, mean_eps: 0.926101\n",
            " 82608/100000: episode: 331, duration: 7.166s, episode steps: 360, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001217, mae: 0.105817, mean_q: 0.142478, mean_eps: 0.925817\n",
            " 82879/100000: episode: 332, duration: 5.403s, episode steps: 271, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001147, mae: 0.101611, mean_q: 0.135718, mean_eps: 0.925532\n",
            " 83257/100000: episode: 333, duration: 7.548s, episode steps: 378, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001381, mae: 0.104492, mean_q: 0.140734, mean_eps: 0.925239\n",
            " 83503/100000: episode: 334, duration: 4.778s, episode steps: 246, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.001112, mae: 0.103247, mean_q: 0.139819, mean_eps: 0.924958\n",
            " 83670/100000: episode: 335, duration: 3.325s, episode steps: 167, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.001683, mae: 0.104449, mean_q: 0.141707, mean_eps: 0.924773\n",
            " 83855/100000: episode: 336, duration: 3.674s, episode steps: 185, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000610, mae: 0.101810, mean_q: 0.137245, mean_eps: 0.924614\n",
            " 84132/100000: episode: 337, duration: 5.561s, episode steps: 277, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.001046, mae: 0.104209, mean_q: 0.139248, mean_eps: 0.924407\n",
            " 84430/100000: episode: 338, duration: 5.866s, episode steps: 298, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000789, mae: 0.101539, mean_q: 0.136369, mean_eps: 0.924148\n",
            " 84793/100000: episode: 339, duration: 7.103s, episode steps: 363, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001097, mae: 0.103222, mean_q: 0.137930, mean_eps: 0.923849\n",
            " 84991/100000: episode: 340, duration: 3.933s, episode steps: 198, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.000568, mae: 0.102796, mean_q: 0.138018, mean_eps: 0.923597\n",
            " 85182/100000: episode: 341, duration: 3.827s, episode steps: 191, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.487 [0.000, 3.000],  loss: 0.001131, mae: 0.099876, mean_q: 0.136213, mean_eps: 0.923423\n",
            " 85357/100000: episode: 342, duration: 3.507s, episode steps: 175, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.001003, mae: 0.103194, mean_q: 0.139046, mean_eps: 0.923257\n",
            " 85539/100000: episode: 343, duration: 3.569s, episode steps: 182, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001629, mae: 0.104494, mean_q: 0.140917, mean_eps: 0.923097\n",
            " 85705/100000: episode: 344, duration: 3.293s, episode steps: 166, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002105, mae: 0.104514, mean_q: 0.140175, mean_eps: 0.922940\n",
            " 85876/100000: episode: 345, duration: 3.415s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 0.001586, mae: 0.105897, mean_q: 0.141128, mean_eps: 0.922789\n",
            " 86121/100000: episode: 346, duration: 4.901s, episode steps: 245, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000794, mae: 0.101724, mean_q: 0.138263, mean_eps: 0.922602\n",
            " 86289/100000: episode: 347, duration: 3.285s, episode steps: 168, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000866, mae: 0.102449, mean_q: 0.140636, mean_eps: 0.922415\n",
            " 86477/100000: episode: 348, duration: 3.692s, episode steps: 188, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.399 [0.000, 3.000],  loss: 0.001417, mae: 0.107348, mean_q: 0.144201, mean_eps: 0.922254\n",
            " 86695/100000: episode: 349, duration: 4.254s, episode steps: 218, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001009, mae: 0.101176, mean_q: 0.137165, mean_eps: 0.922073\n",
            " 86977/100000: episode: 350, duration: 5.609s, episode steps: 282, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000954, mae: 0.104369, mean_q: 0.140290, mean_eps: 0.921848\n",
            " 87226/100000: episode: 351, duration: 4.992s, episode steps: 249, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.001161, mae: 0.103882, mean_q: 0.139159, mean_eps: 0.921608\n",
            " 87492/100000: episode: 352, duration: 5.248s, episode steps: 266, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000907, mae: 0.103834, mean_q: 0.139092, mean_eps: 0.921378\n",
            " 87940/100000: episode: 353, duration: 8.923s, episode steps: 448, steps per second:  50, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000706, mae: 0.100429, mean_q: 0.134301, mean_eps: 0.921057\n",
            " 88117/100000: episode: 354, duration: 3.547s, episode steps: 177, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000958, mae: 0.105966, mean_q: 0.142658, mean_eps: 0.920775\n",
            " 88296/100000: episode: 355, duration: 3.740s, episode steps: 179, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000987, mae: 0.100194, mean_q: 0.134630, mean_eps: 0.920615\n",
            " 88478/100000: episode: 356, duration: 3.743s, episode steps: 182, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000767, mae: 0.103735, mean_q: 0.139577, mean_eps: 0.920453\n",
            " 88789/100000: episode: 357, duration: 6.364s, episode steps: 311, steps per second:  49, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000825, mae: 0.102868, mean_q: 0.138417, mean_eps: 0.920229\n",
            " 88959/100000: episode: 358, duration: 3.407s, episode steps: 170, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000775, mae: 0.099591, mean_q: 0.133755, mean_eps: 0.920013\n",
            " 89166/100000: episode: 359, duration: 4.198s, episode steps: 207, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000991, mae: 0.101351, mean_q: 0.140661, mean_eps: 0.919844\n",
            " 89437/100000: episode: 360, duration: 5.550s, episode steps: 271, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000708, mae: 0.102513, mean_q: 0.137335, mean_eps: 0.919628\n",
            " 89615/100000: episode: 361, duration: 3.608s, episode steps: 178, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000374, mae: 0.101225, mean_q: 0.135384, mean_eps: 0.919427\n",
            " 89799/100000: episode: 362, duration: 3.824s, episode steps: 184, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000633, mae: 0.102976, mean_q: 0.138665, mean_eps: 0.919265\n",
            " 90092/100000: episode: 363, duration: 6.336s, episode steps: 293, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.000705, mae: 0.104382, mean_q: 0.140395, mean_eps: 0.919050\n",
            " 90320/100000: episode: 364, duration: 4.929s, episode steps: 228, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.002345, mae: 0.114969, mean_q: 0.153988, mean_eps: 0.918816\n",
            " 90504/100000: episode: 365, duration: 3.968s, episode steps: 184, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001461, mae: 0.111047, mean_q: 0.149389, mean_eps: 0.918631\n",
            " 90676/100000: episode: 366, duration: 3.660s, episode steps: 172, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001014, mae: 0.109536, mean_q: 0.149116, mean_eps: 0.918471\n",
            " 90950/100000: episode: 367, duration: 5.706s, episode steps: 274, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001329, mae: 0.111136, mean_q: 0.149263, mean_eps: 0.918269\n",
            " 91314/100000: episode: 368, duration: 7.591s, episode steps: 364, steps per second:  48, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001098, mae: 0.111886, mean_q: 0.150292, mean_eps: 0.917981\n",
            " 91576/100000: episode: 369, duration: 5.320s, episode steps: 262, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.000977, mae: 0.109860, mean_q: 0.146656, mean_eps: 0.917700\n",
            " 91872/100000: episode: 370, duration: 5.999s, episode steps: 296, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.001092, mae: 0.110665, mean_q: 0.149110, mean_eps: 0.917450\n",
            " 92287/100000: episode: 371, duration: 8.250s, episode steps: 415, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000488, mae: 0.105963, mean_q: 0.142484, mean_eps: 0.917130\n",
            " 92502/100000: episode: 372, duration: 4.269s, episode steps: 215, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.001027, mae: 0.105231, mean_q: 0.139933, mean_eps: 0.916845\n",
            " 92668/100000: episode: 373, duration: 3.309s, episode steps: 166, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000471, mae: 0.104245, mean_q: 0.140165, mean_eps: 0.916674\n",
            " 92969/100000: episode: 374, duration: 6.089s, episode steps: 301, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000596, mae: 0.108801, mean_q: 0.147022, mean_eps: 0.916464\n",
            " 93247/100000: episode: 375, duration: 5.430s, episode steps: 278, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.000577, mae: 0.108863, mean_q: 0.146748, mean_eps: 0.916203\n",
            " 93623/100000: episode: 376, duration: 7.432s, episode steps: 376, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.000795, mae: 0.108503, mean_q: 0.146426, mean_eps: 0.915909\n",
            " 93842/100000: episode: 377, duration: 4.432s, episode steps: 219, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001439, mae: 0.112202, mean_q: 0.148643, mean_eps: 0.915641\n",
            " 94043/100000: episode: 378, duration: 3.957s, episode steps: 201, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000660, mae: 0.106466, mean_q: 0.141715, mean_eps: 0.915452\n",
            " 94214/100000: episode: 379, duration: 3.377s, episode steps: 171, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.327 [0.000, 3.000],  loss: 0.001242, mae: 0.109424, mean_q: 0.145824, mean_eps: 0.915285\n",
            " 94385/100000: episode: 380, duration: 3.417s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 0.000763, mae: 0.111743, mean_q: 0.150363, mean_eps: 0.915130\n",
            " 94716/100000: episode: 381, duration: 6.495s, episode steps: 331, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.001041, mae: 0.112238, mean_q: 0.151315, mean_eps: 0.914905\n",
            " 94895/100000: episode: 382, duration: 3.552s, episode steps: 179, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.000572, mae: 0.113164, mean_q: 0.152207, mean_eps: 0.914676\n",
            " 95105/100000: episode: 383, duration: 4.200s, episode steps: 210, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000770, mae: 0.108826, mean_q: 0.146375, mean_eps: 0.914500\n",
            " 95282/100000: episode: 384, duration: 3.516s, episode steps: 177, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001021, mae: 0.106569, mean_q: 0.142798, mean_eps: 0.914325\n",
            " 95551/100000: episode: 385, duration: 5.346s, episode steps: 269, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000781, mae: 0.108522, mean_q: 0.145445, mean_eps: 0.914126\n",
            " 95720/100000: episode: 386, duration: 3.391s, episode steps: 169, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.373 [0.000, 3.000],  loss: 0.000861, mae: 0.105478, mean_q: 0.141776, mean_eps: 0.913929\n",
            " 95895/100000: episode: 387, duration: 3.509s, episode steps: 175, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.001012, mae: 0.106045, mean_q: 0.141498, mean_eps: 0.913775\n",
            " 96205/100000: episode: 388, duration: 6.114s, episode steps: 310, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.339 [0.000, 3.000],  loss: 0.000914, mae: 0.114210, mean_q: 0.153874, mean_eps: 0.913555\n",
            " 96377/100000: episode: 389, duration: 3.373s, episode steps: 172, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.000447, mae: 0.105072, mean_q: 0.141591, mean_eps: 0.913337\n",
            " 96732/100000: episode: 390, duration: 7.007s, episode steps: 355, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000479, mae: 0.108421, mean_q: 0.145346, mean_eps: 0.913101\n",
            " 96973/100000: episode: 391, duration: 4.828s, episode steps: 241, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.257 [0.000, 3.000],  loss: 0.001117, mae: 0.106568, mean_q: 0.142062, mean_eps: 0.912833\n",
            " 97245/100000: episode: 392, duration: 5.330s, episode steps: 272, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000646, mae: 0.107535, mean_q: 0.142819, mean_eps: 0.912601\n",
            " 97413/100000: episode: 393, duration: 3.340s, episode steps: 168, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.000558, mae: 0.110754, mean_q: 0.147526, mean_eps: 0.912403\n",
            " 97657/100000: episode: 394, duration: 4.773s, episode steps: 244, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000773, mae: 0.110403, mean_q: 0.147520, mean_eps: 0.912218\n",
            " 97908/100000: episode: 395, duration: 4.973s, episode steps: 251, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000690, mae: 0.104601, mean_q: 0.139228, mean_eps: 0.911996\n",
            " 98255/100000: episode: 396, duration: 6.865s, episode steps: 347, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000850, mae: 0.108714, mean_q: 0.145434, mean_eps: 0.911728\n",
            " 98491/100000: episode: 397, duration: 4.670s, episode steps: 236, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000659, mae: 0.109503, mean_q: 0.148205, mean_eps: 0.911465\n",
            " 98860/100000: episode: 398, duration: 7.327s, episode steps: 369, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000817, mae: 0.108707, mean_q: 0.146148, mean_eps: 0.911193\n",
            " 99094/100000: episode: 399, duration: 4.623s, episode steps: 234, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001115, mae: 0.112208, mean_q: 0.149765, mean_eps: 0.910922\n",
            " 99267/100000: episode: 400, duration: 3.397s, episode steps: 173, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: 0.000738, mae: 0.107579, mean_q: 0.144171, mean_eps: 0.910738\n",
            " 99589/100000: episode: 401, duration: 6.429s, episode steps: 322, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000688, mae: 0.104444, mean_q: 0.139290, mean_eps: 0.910515\n",
            " 99759/100000: episode: 402, duration: 3.642s, episode steps: 170, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.412 [0.000, 3.000],  loss: 0.001487, mae: 0.113311, mean_q: 0.150817, mean_eps: 0.910293\n",
            "done, took 1198.613 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW0US9n4LwFl",
        "outputId": "ad1adfc4-f58e-4cc4-d433-afde15f741f0"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "play.py\n",
        "\"\"\"\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "import tensorflow.keras as K\n",
        "from rl.policy import GreedyQPolicy\n",
        "# create_q_model = __import__('train').create_q_model\n",
        "# AtariProcessor = __import__('train').AtariProcessor\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    # screenshots per state\n",
        "    window = 4  \n",
        "    model = create_q_model(num_actions, window)  \n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
        "                   policy=GreedyQPolicy(),\n",
        "                   processor=processor, memory=memory)\n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "\n",
        "    dqn.load_weights('policy.h5')\n",
        "\n",
        "    # Only works with 'False'\n",
        "    dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 1: reward: 1.000, steps: 10000\n",
            "Episode 2: reward: 0.000, steps: 10000\n",
            "Episode 3: reward: 1.000, steps: 10000\n",
            "Episode 4: reward: 0.000, steps: 10000\n",
            "Episode 5: reward: 0.000, steps: 10000\n",
            "Episode 6: reward: 0.000, steps: 10000\n",
            "Episode 7: reward: 0.000, steps: 10000\n",
            "Episode 8: reward: 0.000, steps: 10000\n",
            "Episode 9: reward: 0.000, steps: 10000\n",
            "Episode 10: reward: 0.000, steps: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}