{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLy3a5kve3dGljJm34Ckkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipeserna/cacharreando/blob/master/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12HP8K2VJBMF"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ua8igzLJKNc"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sRiDnEJRQO"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGj_H0SJWwu",
        "outputId": "9fcabb44-4fc6-45b6-ae17-20a005b7ee9e"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fd8852d4590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85nZ8_g2J7Zd"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars\n",
        "!pip install keras-rl2\n",
        "!pip install gym\n",
        "!pip install gym[atari]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA-xhVElKGJd"
      },
      "source": [
        "# From stackoverflow for displaying\n",
        "env = gym.make(\"Breakout-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCLPqmxSKnW4",
        "outputId": "97459242-6ea3-4824-fce7-e9fbd272908a"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train.py\n",
        "\"\"\"\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras as K\n",
        "from rl.processors import Processor\n",
        "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    \"\"\"\n",
        "    preprocessing\n",
        "    \"\"\"\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\" resizing and grayscale \"\"\"\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize((84, 84), Image.ANTIALIAS).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == (84, 84)\n",
        "        # saves storage in experience memory\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Rescale without using too much memory\n",
        "        \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\" rewards between -1 and 1 \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "\n",
        "def create_q_model(num_actions, window):\n",
        "    \"\"\"\n",
        "    Preprocessing\n",
        "    \"\"\"\n",
        "    # Network\n",
        "\n",
        "    inputs = layers.Input(shape=(window, 84, 84))\n",
        "    # comment the line below to use with GPU\n",
        "    inputs_sort = layers.Permute((2, 3, 1))(inputs)\n",
        "\n",
        "    # Change data_format=\"channels_first\" to use GPU\n",
        "    # change inputs_sort by inputs to use GPU\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(inputs_sort)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\",\n",
        "                           data_format=\"channels_last\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return K.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    window = 4\n",
        "    model = create_q_model(num_actions, window)\n",
        "    model.summary()\n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                                  value_max=1., value_min=.1, value_test=.05,\n",
        "                                  nb_steps=1000000)\n",
        "\n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions, policy=policy,\n",
        "                   memory=memory, processor=processor,\n",
        "                   nb_steps_warmup=50000, gamma=.99,\n",
        "                   target_model_update=10000,\n",
        "                   train_interval=4,\n",
        "                   delta_clip=1.)\n",
        "\n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "    \n",
        "    dqn.fit(env,\n",
        "            nb_steps=5000,\n",
        "            log_interval=10000,\n",
        "            visualize=False,\n",
        "            verbose=2)\n",
        "\n",
        "    dqn.save_weights('policy.h5', overwrite=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 4, 84, 84)]       0         \n",
            "_________________________________________________________________\n",
            "permute_5 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,686,180\n",
            "Trainable params: 1,686,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 5000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  479/5000: episode: 1, duration: 2.276s, episode steps: 479, steps per second: 210, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  659/5000: episode: 2, duration: 0.802s, episode steps: 180, steps per second: 225, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "  873/5000: episode: 3, duration: 0.957s, episode steps: 214, steps per second: 224, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 1055/5000: episode: 4, duration: 0.809s, episode steps: 182, steps per second: 225, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 1450/5000: episode: 5, duration: 1.729s, episode steps: 395, steps per second: 228, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 1628/5000: episode: 6, duration: 0.787s, episode steps: 178, steps per second: 226, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 2001/5000: episode: 7, duration: 1.644s, episode steps: 373, steps per second: 227, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 2213/5000: episode: 8, duration: 0.956s, episode steps: 212, steps per second: 222, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 2499/5000: episode: 9, duration: 1.250s, episode steps: 286, steps per second: 229, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 2844/5000: episode: 10, duration: 1.504s, episode steps: 345, steps per second: 229, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 3025/5000: episode: 11, duration: 0.792s, episode steps: 181, steps per second: 229, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 3347/5000: episode: 12, duration: 1.432s, episode steps: 322, steps per second: 225, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 3519/5000: episode: 13, duration: 0.763s, episode steps: 172, steps per second: 225, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 3818/5000: episode: 14, duration: 1.328s, episode steps: 299, steps per second: 225, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 4037/5000: episode: 15, duration: 0.970s, episode steps: 219, steps per second: 226, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 4261/5000: episode: 16, duration: 0.993s, episode steps: 224, steps per second: 226, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 4487/5000: episode: 17, duration: 0.996s, episode steps: 226, steps per second: 227, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            " 4734/5000: episode: 18, duration: 1.081s, episode steps: 247, steps per second: 229, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "done, took 22.267 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW0US9n4LwFl",
        "outputId": "036e91c8-67cb-46ac-874b-cab40adfde20"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "play.py\n",
        "\"\"\"\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "import tensorflow.keras as K\n",
        "\n",
        "# create_q_model = __import__('train').create_q_model\n",
        "# AtariProcessor = __import__('train').AtariProcessor\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "    env.reset()\n",
        "    num_actions = env.action_space.n\n",
        "    # screenshots per state\n",
        "    window = 4  \n",
        "    model = create_q_model(num_actions, window)  \n",
        "    memory = SequentialMemory(limit=1000000, window_length=window)\n",
        "    processor = AtariProcessor()\n",
        "    dqn = DQNAgent(model=model, nb_actions=num_actions,\n",
        "                   processor=processor, memory=memory)\n",
        "    dqn.compile(K.optimizers.Adam(learning_rate=.00025), metrics=['mae'])\n",
        "\n",
        "    dqn.load_weights('policy.h5')\n",
        "\n",
        "    # Only works with 'False'\n",
        "    dqn.test(env, nb_episodes=4, visualize=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 4 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 1: reward: 0.000, steps: 10000\n",
            "Episode 2: reward: 0.000, steps: 10000\n",
            "Episode 3: reward: 0.000, steps: 10000\n",
            "Episode 4: reward: 0.000, steps: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}